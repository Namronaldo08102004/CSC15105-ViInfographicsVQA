{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a58cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ba2c4",
   "metadata": {},
   "source": [
    "## **1. Crawl Infographics from the TTXVN page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3790db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_TTXVN (base_url: str, start_page: int, end_page: int, num_posts_each_page: int, save_file_name = \"extracted_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on a TTXVN (Vietnam News Agency) infographics website.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the infographics section, e.g., \"https://infographics.vn/the-gioi-t-2.vna\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        num_posts_each_page (int): The expected number of posts listed on each page.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"extracted_links.txt\".\n",
    "    \"\"\"\n",
    "    # Split the base URL to separate the base and the extension.\n",
    "    ext = base_url.split('.')[-1]\n",
    "    base = '.'.join(base_url.split('.')[:-1])\n",
    "\n",
    "    # Initialize an empty list to store the URLs of the pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page based on the page index and the number of posts per page.\n",
    "        url = base + '/' + f'{page_idx}-{num_posts_each_page}-0' + '.' + ext\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Initialize an empty list to store the extracted links.\n",
    "    links = []\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "\n",
    "        # Initialize an empty list to store the final extracted links from the current page.\n",
    "        final_links = []\n",
    "\n",
    "        # Iterate through the temporary list of links to identify and extract relevant links.\n",
    "        for idx in range (len(temp_links)):\n",
    "            # This condition checks for a specific pattern where a \"javascript:void(0);\" link is followed by actual links.\n",
    "            if (temp_links[idx] == \"javascript:void(0);\" and idx + 1 < len(temp_links) and temp_links[idx + 1] != \"javascript:void(0);\"):\n",
    "                # Set a temporary index to traverse the subsequent links.\n",
    "                temp_idx = idx + 1\n",
    "\n",
    "                # Loop through the following links until another \"javascript:void(0);\" is encountered or the end of the list is reached.\n",
    "                while (temp_links[temp_idx] != \"javascript:void(0);\" and temp_idx + 1 < len(temp_links) and temp_links[temp_idx + 1] != \"javascript:void(0);\"):\n",
    "                    # Append the extracted link to the final links list for the current page.\n",
    "                    final_links.append(temp_links[temp_idx])\n",
    "                    # Increment the temporary index to move to the next link.\n",
    "                    temp_idx += 1\n",
    "\n",
    "                # Break out of the inner loop once the relevant links following \"javascript:void(0);\" are processed.\n",
    "                break\n",
    "\n",
    "        # Remove duplicate links by converting the list to a set and back to a list.\n",
    "        final_links = list(set(final_links))\n",
    "        # Filter the links to keep only those with a specific structure (5 parts separated by '/').\n",
    "        final_links = [link for link in final_links if len(link.split('/')) == 5]\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each extracted link to a new line in the file.\n",
    "            for link in final_links:\n",
    "                file.write(link + \"\\n\")\n",
    "\n",
    "        # Extend the main 'links' list with the extracted links from the current page.\n",
    "        links.extend(final_links)\n",
    "\n",
    "        # Introduce a delay to be respectful to the website's server.\n",
    "        sleep(1.5)\n",
    "\n",
    "# Example usage of the function to extract links from pages 0 to 302 of the specified URL.\n",
    "extract_links_TTXVN(\"https://infographics.vn/the-gioi-t-2.vna\", start_page = 0, end_page = 302, num_posts_each_page = 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb901a2",
   "metadata": {},
   "source": [
    "## **2. Crawl Infographics from the VnExpress page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_VnExpress (base_url: str, start_page: int, end_page: int, save_file_name = \"VnExpress_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the VnExpress infographics website.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the infographics section, e.g., \"https://vnexpress.net/infographics\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"VnExpress_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending the page number.\n",
    "        url = base_url + f'-p{page_idx}'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "\n",
    "        # Filter the extracted links to keep only those ending with '.html'.\n",
    "        final_links = [link for link in temp_links if link.endswith('.html')]\n",
    "        # Remove duplicate links by converting the list to a set and back to a list.\n",
    "        final_links = list(set(final_links))\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each extracted link to a new line in the file.\n",
    "            for link in final_links:\n",
    "                file.write(link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 20 of the specified URL.\n",
    "extract_links_VnExpress(\"https://vnexpress.net/infographics\", start_page = 1, end_page = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40133c80",
   "metadata": {},
   "source": [
    "## **3. Crawl Infographics from the Vtv page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d1370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_Vtv (base_url: str, start_page: int, end_page: int, save_file_name = \"Vtv_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the VTV (Vietnam Television) timeline section.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the timeline section, e.g., \"https://vtv.vn/timeline/211/\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"Vtv_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending the page number in the specified format.\n",
    "        url = base_url + f'trang-{page_idx}.htm'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "\n",
    "        # Filter the extracted links to keep only those with a specific structure (3 parts separated by '/').\n",
    "        final_links = [link for link in temp_links if len(link.split('/')) == 3]\n",
    "        # Identify links that appear consecutively and store them in the 'links' list.\n",
    "        links = [final_links[i] for i in range (0, len(final_links) - 1) if final_links[i] == final_links[i + 1]]\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each identified link with the base URL prepended to a new line in the file.\n",
    "            for link in links:\n",
    "                file.write(\"https://vtv.vn\" + link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 50 of the specified URL.\n",
    "extract_links_Vtv(\"https://vtv.vn/timeline/211/\", start_page = 1, end_page = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daecae06",
   "metadata": {},
   "source": [
    "## **4. Crawl Infographics from the Nhan Dan page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_NhanDan (base_url: str, start_page: int, end_page: int, save_file_name = \"NhanDan_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the Nhan Dan (The People) newspaper website's API.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the API endpoint for more news, e.g., \"https://nhandan.vn/api/morenews-zone-1308\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"NhanDan_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the API pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending the page number and other parameters.\n",
    "        url = base_url + f'-{page_idx}.html?phrase=&tag_id=0'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "        # Remove leading/trailing quotes and backslashes from the extracted links.\n",
    "        temp_links = [link.strip('\\\"\\\\') for link in temp_links]\n",
    "\n",
    "        # Filter the extracted links to keep only those ending with '.html'.\n",
    "        final_links = [link for link in temp_links if link.endswith('.html')]\n",
    "        # Filter the links to keep only those with a specific structure (4 parts separated by '/').\n",
    "        final_links = [link for link in final_links if len(link.split('/')) == 4]\n",
    "        # Filter out links that contain the word \"tag\".\n",
    "        final_links = [link for link in final_links if \"tag\" not in link]\n",
    "        # Remove duplicate links by converting the list to a set and back to a list.\n",
    "        final_links = list(set(final_links))\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each extracted link to a new line in the file.\n",
    "            for link in final_links:\n",
    "                file.write(link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 39 of the specified URL.\n",
    "extract_links_NhanDan(\"https://nhandan.vn/api/morenews-zone-1308\", start_page = 1, end_page = 39)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9cb003",
   "metadata": {},
   "source": [
    "## **5. Crawl Infographics from the Chinh Phu page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c58f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_ChinhPhu (base_url: str, start_page: int, end_page: int, save_file_name = \"ChinhPhu_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the Chinh Phu (Government) media infographics timeline.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the infographics timeline section, e.g., \"https://media.chinhphu.vn/timelineinfographics/16/\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"ChinhPhu_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending the page number and \".htm\".\n",
    "        url = base_url + f'{page_idx}.htm'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "\n",
    "        # Remove duplicate links by converting the list to a set and back to a list.\n",
    "        final_links = list(set(temp_links))\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each extracted link with the base domain prepended to a new line in the file.\n",
    "            for link in final_links:\n",
    "                file.write(\"https://media.chinhphu.vn\" + link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 23 of the specified URL.\n",
    "extract_links_ChinhPhu(\"https://media.chinhphu.vn/timelineinfographics/16/\", start_page = 1, end_page = 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4308a",
   "metadata": {},
   "source": [
    "## **6. Crawl Infographics from the Tien Phong page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a56af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_TienPhong (base_url: str, start_page: int, end_page: int, save_file_name = \"TienPhong_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the Tien Phong (Youth) newspaper website's API.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the API endpoint for more news, e.g., \"https://tienphong.vn/api/morenews-zone-287\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"TienPhong_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the API pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending the page number and other parameters.\n",
    "        url = base_url + f'-{page_idx}.html?phrase='\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "        # Remove leading/trailing quotes and backslashes from the extracted links.\n",
    "        temp_links = [link.strip('\\\"\\\\') for link in temp_links]\n",
    "\n",
    "        # Remove duplicate links by converting the list to a set and back to a list.\n",
    "        final_links = list(set(temp_links))\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each extracted link to a new line in the file.\n",
    "            for link in final_links:\n",
    "                file.write(link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 100 of the specified URL.\n",
    "extract_links_TienPhong(\"https://tienphong.vn/api/morenews-zone-287\", start_page = 1, end_page = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ce912",
   "metadata": {},
   "source": [
    "## **7. Crawl Infographics from the Thanh Nien page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1fce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_ThanhNien (base_url: str, start_page: int, end_page: int, save_file_name = \"ThanhNien_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the Thanh Nien (Youth) newspaper's timeline.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the timeline section, e.g., \"https://thanhnien.vn/timelinenewbytype/20/\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"ThanhNien_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending the page number and \".htm\".\n",
    "        url = base_url + f'{page_idx}.htm'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "\n",
    "        # Filter the extracted links to keep only those with at least 3 parts separated by '-'.\n",
    "        final_links = [link for link in temp_links if len(link.split('-')) >= 3]\n",
    "        # Further filter the links to keep only those with exactly 2 parts separated by '/'.\n",
    "        final_links = [link for link in final_links if len(link.split('/')) == 2]\n",
    "\n",
    "        # Identify consecutive duplicate links.\n",
    "        links = [final_links[i] for i in range (0, len(final_links) - 1) if final_links[i] == final_links[i + 1]]\n",
    "        # Remove duplicate links from the identified consecutive links.\n",
    "        links = list(set(links))\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each unique identified link with the base domain prepended to a new line in the file.\n",
    "            for link in links:\n",
    "                file.write(\"https://thanhnien.vn\" + link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 30 of the specified URL.\n",
    "extract_links_ThanhNien(\"https://thanhnien.vn/timelinenewbytype/20/\", start_page = 1, end_page = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a6109",
   "metadata": {},
   "source": [
    "## **8. Crawl Infographics from the Dang Cong San page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_DangCongSan (base_url: str, start_page: int, end_page: int, save_file_name = \"DangCongSan_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the Dang Cong San (Communist Party of Vietnam) website's infographic section.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the infographic section's pagination, e.g., \"https://dangcongsan.vn/multimedia/infographic/p/\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"DangCongSan_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending the page number.\n",
    "        url = base_url + f'{page_idx}'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "\n",
    "        # Filter the extracted links to keep only those with a specific structure (6 parts separated by '/').\n",
    "        final_links = [link for link in temp_links if len(link.split('/')) == 6]\n",
    "        # Filter out links that end with 'index.html'.\n",
    "        final_links = [link for link in final_links if not link.endswith('index.html')]\n",
    "        # Remove duplicate links by converting the list to a set and back to a list.\n",
    "        final_links = list(set(final_links))\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each extracted link to a new line in the file.\n",
    "            for link in final_links:\n",
    "                file.write(link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 39 of the specified URL.\n",
    "extract_links_DangCongSan(\"https://dangcongsan.vn/multimedia/infographic/p/\", start_page = 1, end_page = 39)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a39787c",
   "metadata": {},
   "source": [
    "## **9. Crawl Infographics from the Giao Thong page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_GiaoThong (base_url: str, start_page: int, end_page: int, save_file_name = \"GiaoThong_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the Giao Thong (Transportation) newspaper's timeline.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the timeline section, e.g., \"https://www.baogiaothong.vn/timelinenewbytype/20/\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"GiaoThong_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending the page number and \".htm\".\n",
    "        url = base_url + f'{page_idx}.htm'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "\n",
    "        # Remove duplicate links by converting the list to a set and back to a list.\n",
    "        final_links = list(set(temp_links))\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Iterate through the unique extracted links.\n",
    "            for link in final_links:\n",
    "                # Check if the link starts with 'http' (absolute URL).\n",
    "                if (link.startswith('http')):\n",
    "                    # If it's an absolute URL, write it directly to the file.\n",
    "                    file.write(link + \"\\n\")\n",
    "                else:\n",
    "                    # If it's a relative URL, prepend the base domain and then write it to the file.\n",
    "                    file.write(\"https://www.baogiaothong.vn\" + link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 32 of the specified URL.\n",
    "extract_links_GiaoThong(\"https://www.baogiaothong.vn/timelinenewbytype/20/\", start_page = 1, end_page = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190030f4",
   "metadata": {},
   "source": [
    "## **10. Crawl Infographics from the Sai Gon Giai Phong page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa43905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_SaiGonGiaiPhong (base_url: str, start_page: int, end_page: int, save_file_name = \"SaiGonGiaiPhong_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the Sai Gon Giai Phong (Liberated Saigon) newspaper's API.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the API endpoint for more news, e.g., \"https://api.sggp.org.vn/api/morenews-zone-447\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"SaiGonGiaiPhong_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the API pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending the page number and other parameters.\n",
    "        url = base_url + f'-{page_idx}.html?show_author=1&phrase='\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Parse the JSON response.\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract the 'url' from each item in the 'contents' list within the 'data' dictionary.\n",
    "        temp_links = [item[\"url\"] for item in data[\"data\"][\"contents\"] if \"url\" in item]\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each extracted link with the base domain prepended to a new line in the file.\n",
    "            for link in temp_links:\n",
    "                file.write(\"https://sggp.org.vn\" + link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 46 of the specified URL.\n",
    "extract_links_SaiGonGiaiPhong(\"https://api.sggp.org.vn/api/morenews-zone-447\", start_page = 1, end_page = 46)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86ca40",
   "metadata": {},
   "source": [
    "## **11. Crawl Infographics from the Phap Luat page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901fe2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_PhapLuat (base_url: str, start_page: int, end_page: int, save_file_name = \"PhapLuat_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the Phap Luat (Law) newspaper's API.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the API endpoint for more news, e.g., \"https://api.plo.vn/api/morenews-zone-152\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"PhapLuat_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the API pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending the page number and other parameters.\n",
    "        url = base_url + f'-{page_idx}.html?phrase='\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Parse the JSON response.\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract the 'url' from each item in the 'contents' list within the 'data' dictionary.\n",
    "        temp_links = [item[\"url\"] for item in data[\"data\"][\"contents\"] if \"url\" in item]\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each extracted link with the base domain prepended to a new line in the file.\n",
    "            for link in temp_links:\n",
    "                file.write(\"https://plo.vn\" + link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 104 of the specified URL.\n",
    "extract_links_PhapLuat(\"https://api.plo.vn/api/morenews-zone-152\", start_page = 1, end_page = 104)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18263ac",
   "metadata": {},
   "source": [
    "## **12. Crawl Infographics from the Dan Viet page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc7fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_DanViet (base_url: str, start_page: int, end_page: int, save_file_name = \"DanViet_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the Dan Viet (Vietnamese People) newspaper's timeline.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the timeline section, e.g., \"https://danviet.vn/timelinenewbytype/20/\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"DanViet_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending the page number and \".htm\".\n",
    "        url = base_url + f'{page_idx}.htm'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "        # Remove duplicate links by converting the list to a set.\n",
    "        temp_links = list(set(temp_links))\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each unique extracted link with the base domain prepended to a new line in the file.\n",
    "            for link in temp_links:\n",
    "                file.write(\"https://danviet.vn\" + link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 51 of the specified URL.\n",
    "extract_links_DanViet(\"https://danviet.vn/timelinenewbytype/20/\", start_page = 1, end_page = 51)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8bfe27",
   "metadata": {},
   "source": [
    "## **13. Crawl Infographics from the ZingNews page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_ZingNews (base_url: str, start_page: int, end_page: int, save_file_name = \"ZingNews_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages in the infographic series of Zing News.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the infographic series, e.g., \"https://znews.vn/series/infographic/\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"ZingNews_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending \"trang\" and the page number, followed by \".html\".\n",
    "        url = base_url + f'trang{page_idx}.html'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "        # Remove duplicate links by converting the list to a set.\n",
    "        temp_links = list(set(temp_links))\n",
    "        # Filter the links to keep only those with at least 5 parts separated by '-'.\n",
    "        temp_links = [link for link in temp_links if len(link.split('-')) >= 5]\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Iterate through the filtered unique links.\n",
    "            for link in temp_links:\n",
    "                # Check if the link starts with 'http' (absolute URL).\n",
    "                if (link.startswith('http')):\n",
    "                    # If it's an absolute URL, write it directly to the file.\n",
    "                    file.write(link + \"\\n\")\n",
    "                else:\n",
    "                    # If it's a relative URL, prepend the base domain and then write it to the file.\n",
    "                    file.write(\"https://znews.vn\" + link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 21 of the specified URL.\n",
    "extract_links_ZingNews(\"https://znews.vn/series/infographic/\", start_page = 1, end_page = 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb2e9e",
   "metadata": {},
   "source": [
    "## **14. Crawl Infographics from the Dan Tri page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3408df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_DanTri (base_url: str, start_page: int, end_page: int, save_file_name = \"DanTri_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the Dan Tri (Intellectual People) newspaper's infographic section.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the infographic section, e.g., \"https://dantri.com.vn/infographic/\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"DanTri_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending \"trang-\" and the page number, followed by \".htm\".\n",
    "        url = base_url + f'trang-{page_idx}.htm'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "        # Filter the links to keep only those starting with the Dan Tri base URL.\n",
    "        temp_links = [link for link in temp_links if link.startswith('https://dantri.com.vn')]\n",
    "        # Filter the links to keep only those with at least 8 parts separated by '-'.\n",
    "        temp_links = [link for link in temp_links if len(link.split('-')) >= 8]\n",
    "        # Identify consecutive duplicate links.\n",
    "        temp_links = [temp_links[i] for i in range (0, len(temp_links) - 1) if temp_links[i] == temp_links[i + 1]]\n",
    "        # Remove duplicate links by converting the list to a set.\n",
    "        temp_links = list(set(temp_links))\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each unique extracted link to a new line in the file.\n",
    "            for link in temp_links:\n",
    "                file.write(link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 24 of the specified URL.\n",
    "extract_links_DanTri(\"https://dantri.com.vn/infographic/\", start_page = 1, end_page = 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b21dd",
   "metadata": {},
   "source": [
    "## **15. Crawl Infographics from the An Ninh Thu Do page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2331307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_AnNinhThuDo (base_url: str, start_page: int, end_page: int, save_file_name = \"AnNinhThuDo_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the An Ninh Thu Do (Capital Security) newspaper's API.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the API endpoint for more news, e.g., \"https://api.anninhthudo.vn/api/morenews-type-0\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"AnNinhThuDo_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the API pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending the page number and other parameters.\n",
    "        url = base_url + f'-{page_idx}.html?phrase=&display_type=4&page_size=24'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Parse the JSON response.\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract the 'url' from each item in the 'contents' list within the 'data' dictionary.\n",
    "        temp_links = [item[\"url\"] for item in data[\"data\"][\"contents\"] if \"url\" in item]\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each extracted link with the base domain prepended to a new line in the file.\n",
    "            for link in temp_links:\n",
    "                file.write(\"https://www.anninhthudo.vn\" + link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 125 of the specified URL.\n",
    "extract_links_AnNinhThuDo(\"https://api.anninhthudo.vn/api/morenews-type-0\", start_page = 1, end_page = 125)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c7765c",
   "metadata": {},
   "source": [
    "## **16. Crawl Infographics from the Lao Dong Cong Doan page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef187fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_LaoDongCongDoan (base_url: str, start_page: int, end_page: int, save_file_name = \"LaoDongCongDoan_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the Lao Dong Cong Doan (Labor and Trade Union) newspaper's infographic section.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the infographic section's pagination, e.g., \"https://laodongcongdoan.vn/lao-dong-cong-doan-media/infographic&s_cond=&BRSR=\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"LaoDongCongDoan_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending the page index multiplied by 20.\n",
    "        url = base_url + f'{page_idx * 20}'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "        # Filter the links to keep only those ending with '.html'.\n",
    "        temp_links = [link for link in temp_links if link.endswith('.html')]\n",
    "        # Remove duplicate links by converting the list to a set.\n",
    "        temp_links = list(set(temp_links))\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each unique extracted link to a new line in the file.\n",
    "            for link in temp_links:\n",
    "                file.write(link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 0 to 10 of the specified URL.\n",
    "extract_links_LaoDongCongDoan(\"https://laodongcongdoan.vn/lao-dong-cong-doan-media/infographic&s_cond=&BRSR=\", start_page = 0, end_page = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba0f605",
   "metadata": {},
   "source": [
    "## **17. Crawl Infographics from the Cong Ly page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d610b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main URL for the Cong Ly (Justice) newspaper's infographic section.\n",
    "main = \"https://congly.vn/infographic\"\n",
    "# Define a list of specific API URLs to fetch more infographic articles.\n",
    "URLs = [\"https://congly.vn/api/getMoreArticle/infographic_empty_454972_0_0\",\n",
    "        \"https://congly.vn/api/getMoreArticle/infographic_empty_445554_0_0\",\n",
    "        \"https://congly.vn/api/getMoreArticle/infographic_empty_442045_0_0\",\n",
    "        \"https://congly.vn/api/getMoreArticle/infographic_empty_424912_0_0\",\n",
    "        \"https://congly.vn/api/getMoreArticle/infographic_empty_400985_0_0\",\n",
    "        \"https://congly.vn/api/getMoreArticle/infographic_empty_392777_0_0\",\n",
    "        \"https://congly.vn/api/getMoreArticle/infographic_empty_388026_0_0\",\n",
    "        \"https://congly.vn/api/getMoreArticle/infographic_empty_384411_0_0\",\n",
    "        \"https://congly.vn/api/getMoreArticle/infographic_empty_381213_0_0\",\n",
    "        \"https://congly.vn/api/getMoreArticle/infographic_empty_372665_0_0\",\n",
    "        \"https://congly.vn/api/getMoreArticle/infographic_empty_205427_0_0\",\n",
    "        \"https://congly.vn/api/getMoreArticle/infographic_empty_200381_0_0\",\n",
    "        \"https://congly.vn/api/getMoreArticle/infographic_empty_194916_0_0\",\n",
    "        \"https://congly.vn/api/getMoreArticle/infographic_empty_191154_0_0\",\n",
    "        \"https://congly.vn/api/getMoreArticle/infographic_empty_189627_0_0\",\n",
    "        \"https://congly.vn/api/getMoreArticle/infographic_empty_185388_0_0\",\n",
    "        \"https://congly.vn/api/getMoreArticle/infographic_empty_181024_0_0\"]\n",
    "\n",
    "# Send an HTTP GET request to the main infographic page.\n",
    "response = requests.get(main)\n",
    "# Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "while (response.status_code != 200):\n",
    "    sleep(1.5)\n",
    "    response = requests.get(main)\n",
    "\n",
    "# Get the HTML content of the response.\n",
    "html_content = response.text\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content.\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "# Filter the links to keep only those with at least 8 parts separated by '-'.\n",
    "temp_links = [link for link in temp_links if len(link.split('-')) >= 8]\n",
    "# Remove duplicate links by converting the list to a set.\n",
    "temp_links = list(set(temp_links))\n",
    "\n",
    "# Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "with open(\"CongLy_links.txt\", \"a+\", encoding = \"utf-8\") as file:\n",
    "    # Write each extracted link to a new line in the file.\n",
    "    for link in temp_links:\n",
    "        file.write(link + \"\\n\")\n",
    "\n",
    "# Iterate through the list of API URLs to extract links from the JSON responses.\n",
    "for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "    # Send an HTTP GET request to the current API URL.\n",
    "    response = requests.get(url)\n",
    "    # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "    while (response.status_code != 200):\n",
    "        sleep(1.5)\n",
    "        response = requests.get(url)\n",
    "\n",
    "    # Parse the JSON response.\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the value of the 'LinktoMe2' key from each item in the JSON data.\n",
    "    temp_links = [item[\"LinktoMe2\"] for item in data if \"LinktoMe2\" in item]\n",
    "\n",
    "    # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "    with open(\"CongLy_links.txt\", \"a+\", encoding = \"utf-8\") as file:\n",
    "        # Write each extracted link to a new line in the file.\n",
    "        for link in temp_links:\n",
    "            file.write(link + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01e22f",
   "metadata": {},
   "source": [
    "## **18. Crawl Infographics from the Dau Thau page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620812dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_DauThau (base_url: str, start_page: int, end_page: int, save_file_name = \"DauThau_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the Bao Dau Thau (Vietnam Bidding) newspaper's API,\n",
    "    specifically searching for articles related to \"Infographic\".\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the API endpoint for search, e.g., \"https://api.baodauthau.vn/api/morenews-search-0\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"DauThau_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the API pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending the page number and the search phrase.\n",
    "        url = base_url + f'-{page_idx}.html?phrase=Infographic'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Parse the JSON response.\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract the 'url' from each item in the 'contents' list within the 'data' dictionary.\n",
    "        temp_links = [item[\"url\"] for item in data[\"data\"][\"contents\"] if \"url\" in item]\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each extracted link with the base domain prepended to a new line in the file.\n",
    "            for link in temp_links:\n",
    "                file.write(\"https://baodauthau.vn\" + link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 10 of the specified URL.\n",
    "extract_links_DauThau(\"https://api.baodauthau.vn/api/morenews-search-0\", start_page = 1, end_page = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890b3dff",
   "metadata": {},
   "source": [
    "## **19. Crawl Infographics from the Suc Khoe&Doi Song page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5107f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_SucKhoeVaDoiSong (base_url: str, start_page: int, end_page: int, save_file_name = \"SucKhoeVaDoiSong_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the Suc Khoe Va Doi Song (Health and Life) newspaper's timeline for infographic tags.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the timeline section for infographics, e.g., \"https://suckhoedoisong.vn/timelinetags/infographic/\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"SucKhoeVaDoiSong_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending the page number and \".htm\".\n",
    "        url = base_url + f'{page_idx}.htm'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "        # Filter the links to keep only those with at least 6 parts separated by '-'.\n",
    "        temp_links = [link for link in temp_links if len(link.split('-')) >= 6]\n",
    "        # Remove duplicate links by converting the list to a set.\n",
    "        temp_links = list(set(temp_links))\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each unique extracted link with the base domain prepended to a new line in the file.\n",
    "            for link in temp_links:\n",
    "                file.write(\"https://suckhoedoisong.vn\" + link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 4 of the specified URL.\n",
    "extract_links_SucKhoeVaDoiSong(\"https://suckhoedoisong.vn/timelinetags/infographic/\", start_page = 1, end_page = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dbc82a",
   "metadata": {},
   "source": [
    "## **20. Crawl Infographics from the Gia Dinh&Xa Hoi page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_GiaDinhVaXaHoi (base_url: str, start_page: int, end_page: int, save_file_name = \"GiaDinhVaXaHoi_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the Gia Dinh Va Xa Hoi (Family and Society) newspaper's infographic section.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the infographic section, e.g., \"https://giadinh.suckhoedoisong.vn/multimedia/infographic/\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"GiaDinhVaXaHoi_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending \"trang-\" and the page number, followed by \".htm\".\n",
    "        url = base_url + f'trang-{page_idx}.htm'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "        # Identify consecutive duplicate links.\n",
    "        temp_links = [temp_links[i] for i in range (0, len(temp_links) - 1) if temp_links[i] == temp_links[i + 1]]\n",
    "        # Remove duplicate links by converting the list to a set.\n",
    "        temp_links = list(set(temp_links))\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each unique extracted link with the base domain prepended to a new line in the file.\n",
    "            for link in temp_links:\n",
    "                file.write(\"https://giadinh.suckhoedoisong.vn\" + link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 43 of the specified URL.\n",
    "extract_links_GiaDinhVaXaHoi(\"https://giadinh.suckhoedoisong.vn/multimedia/infographic/\", start_page = 1, end_page = 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18e567",
   "metadata": {},
   "source": [
    "## **21. Crawl Infographics from the Phu Nu page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96fabe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_PhuNu (base_url: str, start_page: int, end_page: int, save_file_name = \"PhuNu_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the Phu Nu Online (Women Online) newspaper's search results for \"InfoGraphic\".\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the search results page, e.g., \"https://www.phunuonline.com.vn/tim-kiem/InfoGraphic.html?cate=&fd=&td=&l=vi&cs=2&ts=3&time=10&sx=1&tim=tin&\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"PhuNu_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending the page number as a query parameter.\n",
    "        url = base_url + f'p={page_idx}'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "        # Filter the links to keep only those starting with 'http' and ending with 'html'.\n",
    "        temp_links = [link for link in temp_links if link.startswith('http') and link.endswith('html')]\n",
    "        # Identify consecutive duplicate links.\n",
    "        temp_links = [temp_links[i] for i in range (0, len(temp_links) - 1) if temp_links[i] == temp_links[i + 1]]\n",
    "        # Remove duplicate links by converting the list to a set.\n",
    "        temp_links = list(set(temp_links))\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each unique extracted link to a new line in the file.\n",
    "            for link in temp_links:\n",
    "                file.write(link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 8 of the specified URL.\n",
    "extract_links_PhuNu(\"https://www.phunuonline.com.vn/tim-kiem/InfoGraphic.html?cate=&fd=&td=&l=vi&cs=2&ts=3&time=10&sx=1&tim=tin&\", start_page = 1, end_page = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f284784",
   "metadata": {},
   "source": [
    "## **22. Crawl Infographics from the Tuoi Tre page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_TuoiTre (base_url: str, start_page: int, end_page: int, save_file_name = \"TuoiTre_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from a range of pages on the Tuoi Tre (Youth) newspaper's infographic section.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the infographic section's pagination, e.g., \"https://tuoitre.vn/ajax-infographic-0/\".\n",
    "        start_page (int): The starting page number to begin extracting links from.\n",
    "        end_page (int): The ending page number to stop extracting links at.\n",
    "        save_file_name (str, optional): The name of the file to save the extracted links to.\n",
    "                                         Defaults to \"TuoiTre_links.txt\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the URLs of the pages to scrape.\n",
    "    URLs = []\n",
    "    # Loop through the specified page range to construct the URLs for each page.\n",
    "    for page_idx in range (start_page, end_page + 1):\n",
    "        # Construct the URL for the current page by appending \"trang-\" and the page number, followed by \".htm\".\n",
    "        url = base_url + f'trang-{page_idx}.htm'\n",
    "        URLs.append(url)\n",
    "\n",
    "    # Iterate through the list of generated URLs to extract links from each page.\n",
    "    for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "        # Send an HTTP GET request to the current URL.\n",
    "        response = requests.get(url)\n",
    "        # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "        while (response.status_code != 200):\n",
    "            sleep(1.5)\n",
    "            response = requests.get(url)\n",
    "\n",
    "        # Get the HTML content of the response.\n",
    "        html_content = response.text\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content.\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "        temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "        # Filter the links to keep only those with at least 6 parts separated by '-'.\n",
    "        temp_links = [link for link in temp_links if len(link.split('-')) >= 6]\n",
    "        # Remove duplicate links by converting the list to a set.\n",
    "        temp_links = list(set(temp_links))\n",
    "\n",
    "        # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "        with open(save_file_name, \"a+\", encoding = \"utf-8\") as file:\n",
    "            # Write each unique extracted link with the base domain prepended to a new line in the file.\n",
    "            for link in temp_links:\n",
    "                file.write(\"https://tuoitre.vn\" + link + \"\\n\")\n",
    "\n",
    "# Example usage of the function to extract links from pages 1 to 95 of the specified URL.\n",
    "extract_links_TuoiTre(\"https://tuoitre.vn/ajax-infographic-0/\", start_page = 1, end_page = 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591c71c",
   "metadata": {},
   "source": [
    "## **23. Crawl Infographics from the VOV page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8688cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_VOV(base_url: str, start_page: int, end_page: int, save_file_name = \"VOV_links.txt\"):\n",
    "    \"\"\"\n",
    "    Extracts links from Infographic pages on VOV (Voice of Vietnam) using Selenium.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the website.\n",
    "        start_page (int): The starting page number.\n",
    "        end_page (int): The ending page number.\n",
    "        save_file_name (str): The name of the file to save the links to. Defaults to \"VOV_links.txt\".\n",
    "    \"\"\"\n",
    "    # Set up Selenium WebDriver (using ChromeDriver)\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode (without opening a browser window)\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    # List of URLs to navigate\n",
    "    URLs = [f\"{base_url}page={page_idx}\" for page_idx in range(start_page, end_page + 1)]\n",
    "\n",
    "    # Open the file for writing\n",
    "    with open(save_file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        for url in tqdm(URLs, desc=\"Extract links\"):\n",
    "            try:\n",
    "                # Access the URL\n",
    "                driver.get(url)\n",
    "                sleep(2)  # Wait for the page to load\n",
    "\n",
    "                # Find all <a> tags with the href attribute\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                temp_links = [link.get_attribute(\"href\") for link in links if link.get_attribute(\"href\")]\n",
    "                # Filter links that have at least 7 parts separated by '-'\n",
    "                temp_links = [link for link in temp_links if len(link.split('-')) >= 7]\n",
    "                # Remove duplicate links\n",
    "                temp_links = list(set(temp_links))\n",
    "\n",
    "                # Write to the file\n",
    "                for link in temp_links:\n",
    "                    file.write(link + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred with URL {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "# Call the function\n",
    "extract_links_VOV(\"https://vov.vn/multimedia/infographic?\", start_page = 0, end_page = 230)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2009075e",
   "metadata": {},
   "source": [
    "## **24. Crawl Infographics from the Dai Doan Ket page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5cf74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main URL for the Dai Doan Ket (Great National Unity) newspaper's infographic section.\n",
    "main = \"https://daidoanket.vn/infographic\"\n",
    "# Define a list of specific API URLs to fetch more infographic articles.\n",
    "URLs = [\"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10288220_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10269319_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10254972_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10243489_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10232658_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10224359_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10221763_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10210339_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10196893_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10185676_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10178345_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10172126_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10168472_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10163409_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10157535_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10152847_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10150152_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10147878_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10146605_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10144379_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10141213_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10137329_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10135225_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10132965_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10131304_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10128459_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10127085_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10125851_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10124410_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10123066_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10121962_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10120877_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10120359_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10119777_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10118840_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10117865_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10116941_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10116097_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10115255_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10114065_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10112765_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10111420_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10110820_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10109363_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10108479_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10107373_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10106637_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10106166_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10105148_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10104362_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10103519_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10102912_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10102180_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10101449_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10101040_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10100456_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10099699_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10098691_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10098121_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10097300_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10096300_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10095872_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10094907_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10093693_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10092740_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10091912_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10091026_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10087166_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10083617_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10079248_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10077426_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10074067_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10070493_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10065477_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10061196_0_0\",\n",
    "        \"https://daidoanket.vn/api/getMoreArticle/infographic_empty_10045667_0_0\"]\n",
    "\n",
    "# Send an HTTP GET request to the main infographic page.\n",
    "response = requests.get(main)\n",
    "# Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "while (response.status_code != 200):\n",
    "    sleep(1.5)\n",
    "    response = requests.get(main)\n",
    "\n",
    "# Get the HTML content of the response.\n",
    "html_content = response.text\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content.\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Find all 'a' tags with the 'href' attribute and extract the link URLs.\n",
    "temp_links = [a.get(\"href\") for a in soup.find_all(\"a\", href = True)]\n",
    "# Identify consecutive duplicate links.\n",
    "temp_links = [temp_links[i] for i in range (0, len(temp_links) - 1) if temp_links[i] == temp_links[i + 1]]\n",
    "# Remove duplicate links by converting the list to a set.\n",
    "temp_links = list(set(temp_links))\n",
    "\n",
    "# Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "with open(\"DaiDoanKet_links.txt\", \"a+\", encoding = \"utf-8\") as file:\n",
    "    # Write each extracted link to a new line in the file.\n",
    "    for link in temp_links:\n",
    "        file.write(link + \"\\n\")\n",
    "\n",
    "# Iterate through the list of API URLs to extract links from the JSON responses.\n",
    "for url in tqdm(URLs, desc = \"Extract links\"):\n",
    "    # Send an HTTP GET request to the current API URL.\n",
    "    response = requests.get(url)\n",
    "    # Implement a retry mechanism in case the request fails (status code is not 200).\n",
    "    while (response.status_code != 200):\n",
    "        sleep(1.5)\n",
    "        response = requests.get(url)\n",
    "\n",
    "    # Parse the JSON response.\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the value of the 'LinktoMe2' key from each item in the JSON data.\n",
    "    temp_links = [item[\"LinktoMe2\"] for item in data if \"LinktoMe2\" in item]\n",
    "\n",
    "    # Open the specified file in append mode with UTF-8 encoding to save the extracted links.\n",
    "    with open(\"DaiDoanKet_links.txt\", \"a+\", encoding = \"utf-8\") as file:\n",
    "        # Write each extracted link to a new line in the file.\n",
    "        for link in temp_links:\n",
    "            file.write(link + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
