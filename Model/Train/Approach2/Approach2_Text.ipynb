{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:42:29.788393Z",
     "iopub.status.busy": "2025-04-19T15:42:29.788071Z",
     "iopub.status.idle": "2025-04-19T15:42:29.792801Z",
     "shell.execute_reply": "2025-04-19T15:42:29.791937Z",
     "shell.execute_reply.started": "2025-04-19T15:42:29.788368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:42:29.794055Z",
     "iopub.status.busy": "2025-04-19T15:42:29.793721Z",
     "iopub.status.idle": "2025-04-19T15:43:02.903140Z",
     "shell.execute_reply": "2025-04-19T15:43:02.902133Z",
     "shell.execute_reply.started": "2025-04-19T15:42:29.794030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch \n",
    "!pip install torchvision\n",
    "!pip install transformers\n",
    "!pip install peft==0.10.0\n",
    "!pip install datasets\n",
    "!pip install pillow\n",
    "!pip install matplotlib\n",
    "!pip install tabulate\n",
    "!pip install underthesea\n",
    "!pip install huggingface_hub\n",
    "!pip install hf_xet\n",
    "!pip install python-dotenv\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-19T15:43:02.905138Z",
     "iopub.status.busy": "2025-04-19T15:43:02.904842Z",
     "iopub.status.idle": "2025-04-19T15:43:05.225303Z",
     "shell.execute_reply": "2025-04-19T15:43:05.224722Z",
     "shell.execute_reply.started": "2025-04-19T15:43:02.905117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "from PIL import ImageFile\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:43:05.226413Z",
     "iopub.status.busy": "2025-04-19T15:43:05.225939Z",
     "iopub.status.idle": "2025-04-19T15:43:05.230590Z",
     "shell.execute_reply": "2025-04-19T15:43:05.229836Z",
     "shell.execute_reply.started": "2025-04-19T15:43:05.226395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define project-specific variables\n",
    "PROJECT_NAME = \"ViInfographicsVQA\"  # Name of the project\n",
    "USERNAME = \"Namronaldo2004\"          # Hugging Face username\n",
    "HUGGINGFACE_HUB_REPO = USERNAME + \"/\" + PROJECT_NAME  # Full repository name on Hugging Face Hub\n",
    "EMAIL = \"thanhdoan0910@gmail.com\"    # Email for repository authentication\n",
    "REPO_ACCESS_TOKEN = os.getenv(\"MODEL_API_KEY\")  # Hugging Face access token\n",
    "BASELINE_NAME = \"Flow3-modified/Text\"\n",
    "CHECKPOINT_FILENAME = f\"{BASELINE_NAME}/latest_checkpoint.pth\"\n",
    "OCR_FILENAME = \"final_OCR.txt\"\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:43:05.232225Z",
     "iopub.status.busy": "2025-04-19T15:43:05.231972Z",
     "iopub.status.idle": "2025-04-19T15:43:05.261553Z",
     "shell.execute_reply": "2025-04-19T15:43:05.261066Z",
     "shell.execute_reply.started": "2025-04-19T15:43:05.232191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import Hugging Face Hub utilities\n",
    "from huggingface_hub.hf_api import HfFolder  # For handling authentication tokens\n",
    "from huggingface_hub import HfApi  # Tools for managing repositories on Hugging Face Hub\n",
    "\n",
    "# Save the Hugging Face authentication atoken\n",
    "HfFolder.save_token(REPO_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:43:05.262302Z",
     "iopub.status.busy": "2025-04-19T15:43:05.262104Z",
     "iopub.status.idle": "2025-04-19T15:43:05.394801Z",
     "shell.execute_reply": "2025-04-19T15:43:05.394308Z",
     "shell.execute_reply.started": "2025-04-19T15:43:05.262287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "CHECKPOINT_PATH = \"./checkpoints/latest_checkpoint.pth\"\n",
    "if api.file_exists(\n",
    "    repo_id = HUGGINGFACE_HUB_REPO, \n",
    "    filename = CHECKPOINT_FILENAME, \n",
    "    repo_type = \"model\"\n",
    "):\n",
    "    CHECKPOINT_PATH = hf_hub_download(\n",
    "        repo_id = HUGGINGFACE_HUB_REPO, \n",
    "        filename = CHECKPOINT_FILENAME, \n",
    "        local_dir = \"./checkpoints\",  # Store the checkpoint locally in the \"checkpoints\" directory\n",
    "        local_dir_use_symlinks = False  # Avoid using symlinks for compatibility\n",
    "    )\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok = True)\n",
    "print(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:43:05.395628Z",
     "iopub.status.busy": "2025-04-19T15:43:05.395405Z",
     "iopub.status.idle": "2025-04-19T15:43:08.089758Z",
     "shell.execute_reply": "2025-04-19T15:43:08.089121Z",
     "shell.execute_reply.started": "2025-04-19T15:43:05.395611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "OCR_CHECKPOINT_PATH = hf_hub_download(\n",
    "    repo_id = HUGGINGFACE_HUB_REPO, \n",
    "    filename = OCR_FILENAME, \n",
    "    local_dir = \"./checkpoints\",  # Store the checkpoint locally in the \"checkpoints\" directory\n",
    "    local_dir_use_symlinks = False  # Avoid using symlinks for compatibility\n",
    ")\n",
    "\n",
    "print(OCR_CHECKPOINT_PATH)\n",
    "\n",
    "ocr_contents = dict()\n",
    "with open (OCR_CHECKPOINT_PATH, 'r', encoding = 'utf-8') as ocr_file:\n",
    "    lines = ocr_file.readlines()\n",
    "    for line in lines:\n",
    "        temp = line.split(\":\")\n",
    "        ocr_index = int(temp[0].split(\" \")[1])\n",
    "        ocr_content = \":\".join(temp[1:]).strip()\n",
    "        ocr_contents[str(ocr_index)] = ocr_content\n",
    "\n",
    "ocr_contents = dict(sorted(ocr_contents.items(), key = lambda x: int(x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:43:08.090701Z",
     "iopub.status.busy": "2025-04-19T15:43:08.090491Z",
     "iopub.status.idle": "2025-04-19T15:43:08.096775Z",
     "shell.execute_reply": "2025-04-19T15:43:08.096051Z",
     "shell.execute_reply.started": "2025-04-19T15:43:08.090686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ViInfographicsVQATextDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for streaming text-image pairs from a Hugging Face dataset.\n",
    "    This dataset yields image-text pairs one by one, without preloading everything into memory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset, max_instances = None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            hf_dataset: The Hugging Face dataset containing image-text pairs.\n",
    "            transform: Optional image transformations (e.g., resizing, normalization).\n",
    "            max_instances: Maximum number of instances to process.\n",
    "        \"\"\"\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.max_instances = max_instances\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterator to stream dataset samples.\n",
    "        \n",
    "        Yields:\n",
    "            A tuple (image, text) where:\n",
    "            - image: Transformed image tensor\n",
    "            - text: Corresponding textual prompt\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        temp_count = 0\n",
    "        for sample in self.hf_dataset:\n",
    "            if (self.max_instances is not None and count >= self.max_instances):\n",
    "                break  # Stop if max instances reached\n",
    "\n",
    "            question_type = sample[\"type\"]\n",
    "            if (question_type.lower() == \"text\"):\n",
    "                ocr_info = ocr_contents[str(temp_count)]\n",
    "                question = sample[\"question\"]\n",
    "                answer = sample[\"answer\"]\n",
    "                \n",
    "                yield ocr_info, question, answer\n",
    "                count += 1  # Increment the counter\n",
    "\n",
    "            temp_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:43:08.097980Z",
     "iopub.status.busy": "2025-04-19T15:43:08.097648Z",
     "iopub.status.idle": "2025-04-19T15:43:08.125397Z",
     "shell.execute_reply": "2025-04-19T15:43:08.124729Z",
     "shell.execute_reply.started": "2025-04-19T15:43:08.097951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BARTPho(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"vinai/bartpho-syllable\",\n",
    "        device=\"cpu\",\n",
    "        max_length=50,\n",
    "        use_lora=False,\n",
    "        lora_r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Load base model\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "        # Apply LoRA if needed\n",
    "        if use_lora:\n",
    "            lora_config = LoraConfig(\n",
    "                r=lora_r,\n",
    "                lora_alpha=lora_alpha,\n",
    "                lora_dropout=lora_dropout,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                target_modules=target_modules\n",
    "            )\n",
    "            base_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "        # Save full model\n",
    "        self.encoder = base_model.base_model.model.model.encoder.to(device)\n",
    "        self.decoder = base_model.base_model.model.model.decoder  # BART/M-BART decoder\n",
    "        self.lm_head = base_model.base_model.model.lm_head\n",
    "\n",
    "    def encode(self, input_texts):\n",
    "        batch_encoder_hidden_states = []\n",
    "        batch_attention_masks = []\n",
    "        batch_input_ids = []\n",
    "    \n",
    "        for text in input_texts:\n",
    "            # Split sentence into chunks (each chunk has at most 512 words)\n",
    "            words = text.split()\n",
    "            chunks = [' '.join(words[i * 384: (i + 1) * 384]) for i in range((len(words) + 383) // 384)]\n",
    "    \n",
    "            chunk_input_ids_list = []\n",
    "            chunk_attention_mask_list = []\n",
    "            chunk_hidden_states_list = []\n",
    "    \n",
    "            for chunk in chunks:\n",
    "                encoded = self.tokenizer(\n",
    "                    chunk,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True\n",
    "                ).to(self.device)\n",
    "    \n",
    "                input_ids = encoded[\"input_ids\"]           # shape: (1, seq_len)\n",
    "                attention_mask = encoded[\"attention_mask\"] # shape: (1, seq_len)\n",
    "    \n",
    "                outputs = self.encoder(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "    \n",
    "                chunk_input_ids_list.append(input_ids)\n",
    "                chunk_attention_mask_list.append(attention_mask)\n",
    "                chunk_hidden_states_list.append(outputs.last_hidden_state)\n",
    "    \n",
    "            # Concatenate all chunks → shape: (1, total_seq_len, hidden_size)\n",
    "            full_input_ids = torch.cat(chunk_input_ids_list, dim=1).squeeze(0)          # shape: (total_seq_len)\n",
    "            full_attention_mask = torch.cat(chunk_attention_mask_list, dim=1).squeeze(0)# shape: (total_seq_len)\n",
    "            full_hidden_states = torch.cat(chunk_hidden_states_list, dim=1).squeeze(0)  # shape: (total_seq_len, hidden_size)\n",
    "    \n",
    "            batch_input_ids.append(full_input_ids)\n",
    "            batch_attention_masks.append(full_attention_mask)\n",
    "            batch_encoder_hidden_states.append(full_hidden_states)\n",
    "    \n",
    "        # Pad sequences in the batch to the same length\n",
    "        max_seq_len = max(x.shape[0] for x in batch_input_ids)\n",
    "    \n",
    "        def pad_tensor(tensor, max_len, pad_value=0):\n",
    "            pad_len = max_len - tensor.shape[0]\n",
    "            if pad_len == 0:\n",
    "                return tensor\n",
    "            pad = torch.full((pad_len,), pad_value, dtype=tensor.dtype, device=tensor.device)\n",
    "            return torch.cat([tensor, pad], dim=0)\n",
    "    \n",
    "        def pad_hidden_states(tensor, max_len):\n",
    "            pad_len = max_len - tensor.shape[0]\n",
    "            if pad_len == 0:\n",
    "                return tensor\n",
    "            pad = torch.zeros((pad_len, tensor.shape[1]), device=tensor.device)\n",
    "            return torch.cat([tensor, pad], dim=0)\n",
    "    \n",
    "        # Stack padded tensors\n",
    "        input_ids = torch.stack([pad_tensor(x, max_seq_len, self.tokenizer.pad_token_id) for x in batch_input_ids], dim=0)\n",
    "        attention_mask = torch.stack([pad_tensor(x, max_seq_len, 0) for x in batch_attention_masks], dim=0)\n",
    "        encoder_hidden_states = torch.stack([pad_hidden_states(x, max_seq_len) for x in batch_encoder_hidden_states], dim=0)\n",
    "    \n",
    "        return {\n",
    "            \"encoder_hidden_states\": encoder_hidden_states,  # shape: (batch_size, seq_len, hidden_size)\n",
    "            \"attention_mask\": attention_mask,                # shape: (batch_size, seq_len)\n",
    "            \"input_ids\": input_ids                           # shape: (batch_size, seq_len)\n",
    "        }\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        answer_input_ids,\n",
    "        answer_attention_mask,\n",
    "        encoder_hidden_states,\n",
    "        encoder_attention_mask,\n",
    "    ):\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=answer_input_ids,\n",
    "            attention_mask=answer_attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "        )\n",
    "        \n",
    "        logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, encoder_hidden_states, encoder_attention_mask):\n",
    "        decoder_input_ids = torch.tensor([[self.tokenizer.eos_token_id]], device=encoder_hidden_states.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(self.max_length):\n",
    "                logits = self.decode(\n",
    "                    answer_input_ids=decoder_input_ids,\n",
    "                    answer_attention_mask=torch.ones_like(decoder_input_ids).to(encoder_hidden_states.device),\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask\n",
    "                )\n",
    "\n",
    "                next_token = logits[:, -1, :].argmax(-1, keepdim=True)\n",
    "                decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=-1)\n",
    "\n",
    "        return self.tokenizer.batch_decode(decoder_input_ids, skip_special_tokens=True)\n",
    "\n",
    "    def freeze_encoder(self, layers_to_freeze=None):\n",
    "        if layers_to_freeze is None:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for idx, layer in enumerate(self.encoder.layers):\n",
    "                if idx in layers_to_freeze:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def unfreeze_encoder(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:43:08.126489Z",
     "iopub.status.busy": "2025-04-19T15:43:08.126174Z",
     "iopub.status.idle": "2025-04-19T15:43:08.151912Z",
     "shell.execute_reply": "2025-04-19T15:43:08.151444Z",
     "shell.execute_reply.started": "2025-04-19T15:43:08.126464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model = 768, d_ff = 2048, dropout = 0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dropout1(F.gelu(self.fc1(x)))\n",
    "        out = self.dropout2(self.fc2(out))\n",
    "        return self.norm(x + out)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model = 768, num_heads = 8, dropout = 0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim = d_model, num_heads = num_heads, dropout = dropout, batch_first = True)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask=None):\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = ~attention_mask.bool()  # Convert to padding mask: True = MASK\n",
    "        attn_output, _ = self.attn(queries, keys, values, key_padding_mask = attention_mask)\n",
    "        out = self.dropout(attn_output)\n",
    "        return self.norm(queries + out)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model = 768, num_heads = 8, d_ff = 2048, dropout = 0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mhatt = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.pwff = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask = None):\n",
    "        att = self.mhatt(queries, keys, values, attention_mask)\n",
    "        ff = self.pwff(att)\n",
    "        return ff\n",
    "\n",
    "class CoAttentionEncoder(nn.Module):\n",
    "    def __init__(self, d_model = 1024, num_heads = 8, d_ff = 2048, dropout = 0.1, num_layers = 3, max_len = 6000):\n",
    "        super(CoAttentionEncoder, self).__init__()\n",
    "\n",
    "        self.vision_pos_embed = nn.Embedding(max_len, d_model)\n",
    "        self.text_pos_embed = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        self.vision_norm = nn.LayerNorm(d_model)\n",
    "        self.text_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.d_model = d_model  # D = 1024\n",
    "        # self.text_proj = nn.Linear(1024, self.d_model)  # ❌ Loại bỏ vì không cần nữa\n",
    "\n",
    "        self.vision_language_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.language_vision_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.vision_self_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.language_self_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, vision_feats, vision_mask, text_feats, text_mask):\n",
    "        batch_size, v_len, _ = vision_feats.size()\n",
    "        _, t_len, _ = text_feats.size()\n",
    "\n",
    "        v_pos_ids = torch.arange(v_len, device=vision_feats.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        t_pos_ids = torch.arange(t_len, device=text_feats.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "        vision_feats = self.vision_norm(vision_feats + self.vision_pos_embed(v_pos_ids))\n",
    "        text_feats = self.text_norm(text_feats + self.text_pos_embed(t_pos_ids))\n",
    "\n",
    "        for vl_attn, lv_attn, v_self, l_self in zip(\n",
    "            self.vision_language_attn_layers,\n",
    "            self.language_vision_attn_layers,\n",
    "            self.vision_self_attn_layers,\n",
    "            self.language_self_attn_layers\n",
    "        ):\n",
    "            vision_feats = vl_attn(vision_feats, text_feats, text_feats, text_mask)\n",
    "            text_feats = lv_attn(text_feats, vision_feats, vision_feats, vision_mask)\n",
    "\n",
    "            vision_feats = v_self(vision_feats, vision_feats, vision_feats, vision_mask)\n",
    "            text_feats = l_self(text_feats, text_feats, text_feats, text_mask)\n",
    "\n",
    "        fused_feats = torch.cat([vision_feats, text_feats], dim=1)  # shape: (B, V+T, D)\n",
    "\n",
    "        return fused_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:43:08.154217Z",
     "iopub.status.busy": "2025-04-19T15:43:08.153955Z",
     "iopub.status.idle": "2025-04-19T15:43:08.180176Z",
     "shell.execute_reply": "2025-04-19T15:43:08.179705Z",
     "shell.execute_reply.started": "2025-04-19T15:43:08.154202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextModel, self).__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.bart_pho = BARTPho(device = self.device, use_lora = True)  # truyền rõ device\n",
    "        self.bart_pho.freeze_encoder()\n",
    "        self.encoder = CoAttentionEncoder().to(self.device)\n",
    "\n",
    "    def forward(self, ocr_infos, questions, answers):\n",
    "        ocr_encoding = self.bart_pho.encode(ocr_infos)\n",
    "        ocr_feats = ocr_encoding[\"encoder_hidden_states\"]\n",
    "        ocr_attention_mask = ocr_encoding[\"attention_mask\"]\n",
    "    \n",
    "        question_encoding = self.bart_pho.encode(questions)\n",
    "        question_feats = question_encoding[\"encoder_hidden_states\"]\n",
    "        question_attention_mask = question_encoding[\"attention_mask\"]\n",
    "    \n",
    "        encoder_output = self.encoder(ocr_feats, ocr_attention_mask, question_feats, question_attention_mask)\n",
    "        encoder_attention_mask = torch.cat([ocr_attention_mask, question_attention_mask], dim = 1)\n",
    "    \n",
    "        answer_encoded = self.bart_pho.tokenizer(\n",
    "            answers, return_tensors = \"pt\", padding = True, truncation = True\n",
    "        ).to(self.device)\n",
    "        answer_input_ids = answer_encoded[\"input_ids\"]\n",
    "        answer_attention_mask = answer_encoded[\"attention_mask\"]\n",
    "    \n",
    "        logits = self.bart_pho.decode(\n",
    "            answer_input_ids = answer_input_ids,\n",
    "            answer_attention_mask = answer_attention_mask,\n",
    "            encoder_hidden_states = encoder_output,\n",
    "            encoder_attention_mask = encoder_attention_mask\n",
    "        )\n",
    "    \n",
    "        return logits\n",
    "\n",
    "    def generate(self, ocr_infos, questions):\n",
    "         # Step 1: Extract visual features\n",
    "        with torch.no_grad():\n",
    "            ocr_encoding = self.bart_pho.encode(ocr_infos)\n",
    "            ocr_feats = ocr_encoding[\"encoder_hidden_states\"]\n",
    "            ocr_attention_mask = ocr_encoding[\"attention_mask\"]\n",
    "        \n",
    "            question_encoding = self.bart_pho.encode(questions)\n",
    "            question_feats = question_encoding[\"encoder_hidden_states\"]\n",
    "            question_attention_mask = question_encoding[\"attention_mask\"]\n",
    "        \n",
    "            encoder_output = self.encoder(ocr_feats, ocr_attention_mask, question_feats, question_attention_mask)\n",
    "            encoder_attention_mask = torch.cat([ocr_attention_mask, question_attention_mask], dim = 1)\n",
    "    \n",
    "            return self.bart_pho.generate(encoder_output, encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:43:08.181640Z",
     "iopub.status.busy": "2025-04-19T15:43:08.180871Z",
     "iopub.status.idle": "2025-04-19T15:43:08.205947Z",
     "shell.execute_reply": "2025-04-19T15:43:08.205456Z",
     "shell.execute_reply.started": "2025-04-19T15:43:08.181622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VQATrainer:\n",
    "    def __init__(self, model, train_loader, criterion, optimizer, lr_scheduler):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Store training loss values for monitoring\n",
    "        self.train_losses = []\n",
    "\n",
    "    def train(self, start_epoch = 0, num_epochs = NUM_EPOCHS):\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(start_epoch, min(start_epoch + num_epochs, NUM_EPOCHS)):\n",
    "            total_loss = 0.0\n",
    "            pbar = tqdm(self.train_loader, desc = f\"[Epoch {epoch+1}/{NUM_EPOCHS}] Training\", leave = False)\n",
    "            numDatas = 0\n",
    "\n",
    "            for batch in pbar:\n",
    "                ocr_infos, questions, answers = batch\n",
    "                # Forward pass\n",
    "                logits = self.model(ocr_infos, questions, answers)\n",
    "\n",
    "                answer_encoded = self.model.bart_pho.tokenizer(\n",
    "                    answers, return_tensors=\"pt\", padding=True, truncation=True\n",
    "                )\n",
    "                answer_input_ids = answer_encoded[\"input_ids\"].to(self.device)\n",
    "                labels = answer_input_ids\n",
    "                \n",
    "                loss = self.criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix(loss = loss.item())\n",
    "                numDatas += 1\n",
    "\n",
    "            self.lr_scheduler.step()\n",
    "            avg_loss = total_loss / numDatas\n",
    "            print(f\"[Epoch {epoch + 1}] Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            #\n",
    "            self.train_losses.append(avg_loss)\n",
    "\n",
    "            #\n",
    "            self.save_checkpoint(epoch, CHECKPOINT_PATH)\n",
    "\n",
    "            api = HfApi()\n",
    "            api.upload_file(\n",
    "                path_or_fileobj = CHECKPOINT_PATH,                      # File trên máy\n",
    "                path_in_repo = CHECKPOINT_FILENAME,              # File sẽ nằm trong thư mục mới\n",
    "                repo_id = HUGGINGFACE_HUB_REPO,                        # Repo đích\n",
    "                repo_type = \"model\",                                      # Hoặc \"model\"\n",
    "                commit_message = f\"Completed training {BASELINE_NAME} until epoch {epoch + 1}!\"\n",
    "            )\n",
    "\n",
    "        print(\"Completed training!\")\n",
    "\n",
    "    def save_checkpoint(self, epoch, filepath):\n",
    "        # Save model checkpoint with necessary states\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.lr_scheduler.state_dict(),\n",
    "            'losses': self.train_losses\n",
    "        }\n",
    "        if os.path.exists(filepath):\n",
    "            os.remove(filepath)\n",
    "\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Checkpoint saved at {filepath}\")\n",
    "    \n",
    "    def load_checkpoint(self, filepath):\n",
    "        # Load checkpoint and restore model state\n",
    "        if os.path.isfile(filepath):\n",
    "            checkpoint = torch.load(filepath, map_location = self.device)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            self.train_losses = checkpoint['losses']\n",
    "            \n",
    "            print(f\"Loaded checkpoint from {filepath}, resuming from epoch {start_epoch + 1}\")\n",
    "            return start_epoch\n",
    "        else:\n",
    "            print(f\"No checkpoint found at {filepath}, starting from scratch\")\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:43:08.206672Z",
     "iopub.status.busy": "2025-04-19T15:43:08.206514Z",
     "iopub.status.idle": "2025-04-19T15:43:09.705378Z",
     "shell.execute_reply": "2025-04-19T15:43:09.704855Z",
     "shell.execute_reply.started": "2025-04-19T15:43:08.206660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_hf_dataset = load_dataset(\"Namronaldo2004/ViInfographicsVQA\", split = \"train\", streaming = True)\n",
    "train_dataset = ViInfographicsVQATextDataset(train_hf_dataset)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    ocr_infos, questions, answers = zip(*batch)\n",
    "    \n",
    "    ocr_infos = list(ocr_infos)\n",
    "    questions = list(questions)\n",
    "    answers = list(answers)\n",
    "    \n",
    "    return ocr_infos, questions, answers\n",
    "    \n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 2, shuffle = False, collate_fn = custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:43:09.706387Z",
     "iopub.status.busy": "2025-04-19T15:43:09.706136Z",
     "iopub.status.idle": "2025-04-19T15:43:22.398866Z",
     "shell.execute_reply": "2025-04-19T15:43:22.397795Z",
     "shell.execute_reply.started": "2025-04-19T15:43:09.706370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Khởi tạo model\n",
    "VQA_model = TextModel()\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = VQA_model.bart_pho.tokenizer.pad_token_id)\n",
    "\n",
    "# Optimizer và scheduler\n",
    "optimizer = optim.AdamW(VQA_model.parameters(), lr = 1e-8)\n",
    "lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.9)\n",
    "\n",
    "# Khởi tạo class Trainer\n",
    "VQA_trainer = VQATrainer(VQA_model, train_dataloader, criterion, optimizer, lr_scheduler)\n",
    "start_epoch = VQA_trainer.load_checkpoint(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:43:22.400822Z",
     "iopub.status.busy": "2025-04-19T15:43:22.400497Z",
     "iopub.status.idle": "2025-04-19T15:43:30.805324Z",
     "shell.execute_reply": "2025-04-19T15:43:30.804488Z",
     "shell.execute_reply.started": "2025-04-19T15:43:22.400793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample = next(iter(train_hf_dataset))\n",
    "ocr_info = ocr_contents[\"0\"]\n",
    "question = sample[\"question\"]\n",
    "\n",
    "VQA_model.generate([ocr_info], [question])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-19T15:50:11.142Z",
     "iopub.execute_input": "2025-04-19T15:43:30.806487Z",
     "iopub.status.busy": "2025-04-19T15:43:30.806215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "VQA_trainer.train(start_epoch = start_epoch, num_epochs = 2)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
