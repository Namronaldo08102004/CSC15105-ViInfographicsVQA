{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:32:08.627681Z",
     "iopub.status.busy": "2025-04-18T13:32:08.627188Z",
     "iopub.status.idle": "2025-04-18T13:32:08.638651Z",
     "shell.execute_reply": "2025-04-18T13:32:08.637901Z",
     "shell.execute_reply.started": "2025-04-18T13:32:08.627629Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:32:08.639975Z",
     "iopub.status.busy": "2025-04-18T13:32:08.639753Z",
     "iopub.status.idle": "2025-04-18T13:34:44.069962Z",
     "shell.execute_reply": "2025-04-18T13:34:44.069179Z",
     "shell.execute_reply.started": "2025-04-18T13:32:08.639958Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install salesforce-lavis\n",
    "!pip install torch \n",
    "!pip install torchvision\n",
    "!pip install transformers\n",
    "!pip install peft==0.10.0\n",
    "!pip install datasets\n",
    "!pip install pillow\n",
    "!pip install matplotlib\n",
    "!pip install tabulate\n",
    "!pip install underthesea\n",
    "!pip install huggingface_hub\n",
    "!pip install hf_xet\n",
    "!pip install python-dotenv\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-18T13:34:44.071611Z",
     "iopub.status.busy": "2025-04-18T13:34:44.071358Z",
     "iopub.status.idle": "2025-04-18T13:35:14.409702Z",
     "shell.execute_reply": "2025-04-18T13:35:14.408932Z",
     "shell.execute_reply.started": "2025-04-18T13:34:44.071588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from lavis.models import load_model_and_preprocess\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:35:14.411331Z",
     "iopub.status.busy": "2025-04-18T13:35:14.410656Z",
     "iopub.status.idle": "2025-04-18T13:35:14.415691Z",
     "shell.execute_reply": "2025-04-18T13:35:14.415010Z",
     "shell.execute_reply.started": "2025-04-18T13:35:14.411302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define project-specific variables\n",
    "PROJECT_NAME = \"ViInfographicsVQA\"  # Name of the project\n",
    "USERNAME = \"Namronaldo2004\"          # Hugging Face username\n",
    "HUGGINGFACE_HUB_REPO = USERNAME + \"/\" + PROJECT_NAME  # Full repository name on Hugging Face Hub\n",
    "REPO_ACCESS_TOKEN = os.getenv(\"MODEL_API_KEY\")  # Hugging Face access token\n",
    "BASELINE_NAME = \"Flow1-modified\"\n",
    "CHECKPOINT_FILENAME = f\"{BASELINE_NAME}/latest_checkpoint.pth\"\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:35:14.417611Z",
     "iopub.status.busy": "2025-04-18T13:35:14.417375Z",
     "iopub.status.idle": "2025-04-18T13:35:14.441125Z",
     "shell.execute_reply": "2025-04-18T13:35:14.440382Z",
     "shell.execute_reply.started": "2025-04-18T13:35:14.417595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import Hugging Face Hub utilities\n",
    "from huggingface_hub.hf_api import HfFolder  # For handling authentication tokens\n",
    "from huggingface_hub import HfApi  # Tools for managing repositories on Hugging Face Hub\n",
    "\n",
    "# Save the Hugging Face authentication atoken\n",
    "HfFolder.save_token(REPO_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:35:14.441922Z",
     "iopub.status.busy": "2025-04-18T13:35:14.441755Z",
     "iopub.status.idle": "2025-04-18T13:35:14.540650Z",
     "shell.execute_reply": "2025-04-18T13:35:14.540021Z",
     "shell.execute_reply.started": "2025-04-18T13:35:14.441908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "CHECKPOINT_PATH = \"./checkpoints/latest_checkpoint.pth\"\n",
    "if api.file_exists(\n",
    "    repo_id = HUGGINGFACE_HUB_REPO, \n",
    "    filename = CHECKPOINT_FILENAME, \n",
    "    repo_type = \"model\"\n",
    "):\n",
    "    CHECKPOINT_PATH = hf_hub_download(\n",
    "        repo_id = HUGGINGFACE_HUB_REPO, \n",
    "        filename = CHECKPOINT_FILENAME, \n",
    "        local_dir = \"./checkpoints\",  # Store the checkpoint locally in the \"checkpoints\" directory\n",
    "        local_dir_use_symlinks = False  # Avoid using symlinks for compatibility\n",
    "    )\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok = True)\n",
    "print(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:35:14.541480Z",
     "iopub.status.busy": "2025-04-18T13:35:14.541277Z",
     "iopub.status.idle": "2025-04-18T13:35:14.547219Z",
     "shell.execute_reply": "2025-04-18T13:35:14.546504Z",
     "shell.execute_reply.started": "2025-04-18T13:35:14.541464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ViInfographicsVQADataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for streaming text-image pairs from a Hugging Face dataset.\n",
    "    This dataset yields image-text pairs one by one, without preloading everything into memory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset,\n",
    "                 transform = None, max_instances = None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            hf_dataset: The Hugging Face dataset containing image-text pairs.\n",
    "            transform: Optional image transformations (e.g., resizing, normalization).\n",
    "            max_instances: Maximum number of instances to process.\n",
    "        \"\"\"\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "        self.max_instances = max_instances\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterator to stream dataset samples.\n",
    "        \n",
    "        Yields:\n",
    "            A tuple (image, text) where:\n",
    "            - image: Transformed image tensor\n",
    "            - text: Corresponding textual prompt\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for sample in self.hf_dataset:\n",
    "            if (self.max_instances is not None and count >= self.max_instances):\n",
    "                break  # Stop if max instances reached\n",
    "            \n",
    "            image = sample[\"image\"]  # Load image from dataset\n",
    "            question = sample[\"question\"]\n",
    "            answer = sample[\"answer\"]\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)  # Apply transformations if provided\n",
    "            \n",
    "            yield image, question, answer\n",
    "            count += 1  # Increment the counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:35:14.548196Z",
     "iopub.status.busy": "2025-04-18T13:35:14.547935Z",
     "iopub.status.idle": "2025-04-18T13:35:14.566175Z",
     "shell.execute_reply": "2025-04-18T13:35:14.565555Z",
     "shell.execute_reply.started": "2025-04-18T13:35:14.548165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EfficientNetFeatureExtractor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"efficientnet_b0\",\n",
    "        target_size: int = 224,\n",
    "        central_fraction: float = 0.875,\n",
    "    ):\n",
    "        super(EfficientNetFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self._load_model(model_name).to(self.device)\n",
    "        self.transform = self._build_transform(target_size, central_fraction)\n",
    "\n",
    "        self.pooling1 = nn.AdaptiveAvgPool2d((1, 32))\n",
    "        self.pooling2 = nn.AdaptiveAvgPool2d((1, 1024))\n",
    "\n",
    "    def _load_model(self, model_name: str) -> nn.Module:\n",
    "        model_dict = {\n",
    "            \"efficientnet_b0\": models.efficientnet_b0,\n",
    "            \"efficientnet_b1\": models.efficientnet_b1,\n",
    "            \"efficientnet_b2\": models.efficientnet_b2,\n",
    "            \"efficientnet_b3\": models.efficientnet_b3,\n",
    "            \"efficientnet_b4\": models.efficientnet_b4,\n",
    "            \"efficientnet_b5\": models.efficientnet_b5,\n",
    "            \"efficientnet_b6\": models.efficientnet_b6,\n",
    "            \"efficientnet_b7\": models.efficientnet_b7,\n",
    "        }\n",
    "\n",
    "        if model_name not in model_dict:\n",
    "            raise ValueError(f\"Unsupported model_name '{model_name}'. Choose from: {list(model_dict.keys())}\")\n",
    "\n",
    "        model = model_dict[model_name](weights=\"DEFAULT\")\n",
    "        return model.features  # Only use the feature extractor part\n",
    "\n",
    "    def _build_transform(self, target_size: int, central_fraction: float) -> nn.Sequential:\n",
    "        resize_size = int(target_size / central_fraction)\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(resize_size),\n",
    "            transforms.CenterCrop(target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
    "                                 std = [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def freeze(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, images: Image.Image) -> torch.Tensor:\n",
    "        images_tensor = torch.stack([\n",
    "            self.transform(image.convert(\"RGB\")) for image in images\n",
    "        ]).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = self.model(images_tensor)\n",
    "\n",
    "        features = self.pooling1(features)\n",
    "        features = features.permute(0, 3, 2, 1)\n",
    "        features = self.pooling2(features)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        flattened = features.view(batch_size, features.shape[1], -1)\n",
    "\n",
    "        return flattened\n",
    "\n",
    "class Blip2ViTExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Blip2ViTExtractor, self).__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model, self.preprocess, _ = load_model_and_preprocess(\n",
    "            name=\"blip2_feature_extractor\",\n",
    "            model_type=\"pretrain\",\n",
    "            is_eval=True,\n",
    "            device=self.device\n",
    "        )\n",
    "        self.preprocess = self.preprocess[\"eval\"]\n",
    "\n",
    "        # üëá Th√™m l·ªõp Linear ƒë·ªÉ chuy·ªÉn t·ª´ 768 ‚Üí 1024\n",
    "        self.linear_proj = nn.Linear(768, 1024)\n",
    "\n",
    "    def freeze(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, images):\n",
    "        images = torch.stack([\n",
    "            self.preprocess(image.convert(\"RGB\")).to(self.device)\n",
    "            for image in images\n",
    "        ])\n",
    "\n",
    "        image_features = self.model.extract_features(\n",
    "            samples={\"image\": images},\n",
    "            mode=\"image\"\n",
    "        ).image_embeds  # shape: (B, N, 768)\n",
    "\n",
    "        image_features = self.linear_proj(image_features)  # shape: (B, N, 1024)\n",
    "        return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:35:14.567637Z",
     "iopub.status.busy": "2025-04-18T13:35:14.566915Z",
     "iopub.status.idle": "2025-04-18T13:35:14.587020Z",
     "shell.execute_reply": "2025-04-18T13:35:14.586392Z",
     "shell.execute_reply.started": "2025-04-18T13:35:14.567612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BARTpho(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"vinai/bartpho-syllable\",\n",
    "        device=\"cpu\",\n",
    "        max_length=50,\n",
    "        use_lora=False,\n",
    "        lora_r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Load base model\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "        # Apply LoRA if needed\n",
    "        if use_lora:\n",
    "            lora_config = LoraConfig(\n",
    "                r=lora_r,\n",
    "                lora_alpha=lora_alpha,\n",
    "                lora_dropout=lora_dropout,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                target_modules=target_modules\n",
    "            )\n",
    "            base_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "        # Save full model\n",
    "        self.encoder = base_model.base_model.model.model.encoder.to(device)\n",
    "        self.decoder = base_model.base_model.model.model.decoder  # BART/M-BART decoder\n",
    "        self.lm_head = base_model.base_model.model.lm_head\n",
    "\n",
    "    def encode(self, input_texts):\n",
    "        encoded = self.tokenizer(\n",
    "            input_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        input_ids = encoded[\"input_ids\"]\n",
    "        attention_mask = encoded[\"attention_mask\"]\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"encoder_hidden_states\": encoder_outputs.last_hidden_state,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"input_ids\": input_ids\n",
    "        }\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        answer_input_ids,\n",
    "        answer_attention_mask,\n",
    "        encoder_hidden_states,\n",
    "        encoder_attention_mask,\n",
    "    ):\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=answer_input_ids,\n",
    "            attention_mask=answer_attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "        )\n",
    "        \n",
    "        logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, encoder_hidden_states, encoder_attention_mask):\n",
    "        decoder_input_ids = torch.tensor([[self.tokenizer.eos_token_id]], device=encoder_hidden_states.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(self.max_length):\n",
    "                logits = self.decode(\n",
    "                    answer_input_ids=decoder_input_ids,\n",
    "                    answer_attention_mask=torch.ones_like(decoder_input_ids).to(encoder_hidden_states.device),\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask\n",
    "                )\n",
    "\n",
    "                next_token = logits[:, -1, :].argmax(-1, keepdim=True)\n",
    "                decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=-1)\n",
    "\n",
    "        return self.tokenizer.batch_decode(decoder_input_ids, skip_special_tokens=True)\n",
    "\n",
    "    def freeze_encoder(self, layers_to_freeze=None):\n",
    "        if layers_to_freeze is None:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for idx, layer in enumerate(self.encoder.layers):\n",
    "                if idx in layers_to_freeze:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def unfreeze_encoder(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:35:14.588031Z",
     "iopub.status.busy": "2025-04-18T13:35:14.587783Z",
     "iopub.status.idle": "2025-04-18T13:35:14.616596Z",
     "shell.execute_reply": "2025-04-18T13:35:14.615878Z",
     "shell.execute_reply.started": "2025-04-18T13:35:14.588010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model = 768, d_ff = 2048, dropout = 0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dropout1(F.gelu(self.fc1(x)))\n",
    "        out = self.dropout2(self.fc2(out))\n",
    "        return self.norm(x + out)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model = 768, num_heads = 8, dropout = 0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim = d_model, num_heads = num_heads, dropout = dropout, batch_first = True)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask=None):\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = ~attention_mask.bool()  # Convert to padding mask: True = MASK\n",
    "        attn_output, _ = self.attn(queries, keys, values, key_padding_mask = attention_mask)\n",
    "        out = self.dropout(attn_output)\n",
    "        return self.norm(queries + out)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model = 768, num_heads = 8, d_ff = 2048, dropout = 0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mhatt = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.pwff = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask = None):\n",
    "        att = self.mhatt(queries, keys, values, attention_mask)\n",
    "        ff = self.pwff(att)\n",
    "        return ff\n",
    "\n",
    "class BiDirectionalCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model = 1024, num_heads = 8, d_ff = 2048, dropout = 0.1, num_layers = 3, max_len = 1028):\n",
    "        super(BiDirectionalCrossAttention, self).__init__()\n",
    "\n",
    "        self.vision_pos_embed = nn.Embedding(max_len, d_model)\n",
    "        self.text_pos_embed = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        self.vision_norm = nn.LayerNorm(d_model)\n",
    "        self.text_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.d_model = d_model  # D = 1024\n",
    "        # self.text_proj = nn.Linear(1024, self.d_model)  # ‚ùå Lo·∫°i b·ªè v√¨ kh√¥ng c·∫ßn n·ªØa\n",
    "\n",
    "        self.vision_language_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.language_vision_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.vision_self_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.language_self_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, vision_feats, vision_mask, text_feats, text_mask):\n",
    "        batch_size, v_len, _ = vision_feats.size()\n",
    "        _, t_len, _ = text_feats.size()\n",
    "\n",
    "        v_pos_ids = torch.arange(v_len, device=vision_feats.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        t_pos_ids = torch.arange(t_len, device=text_feats.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "        vision_feats = self.vision_norm(vision_feats + self.vision_pos_embed(v_pos_ids))\n",
    "        text_feats = self.text_norm(text_feats + self.text_pos_embed(t_pos_ids))\n",
    "\n",
    "        for vl_attn, lv_attn, v_self, l_self in zip(\n",
    "            self.vision_language_attn_layers,\n",
    "            self.language_vision_attn_layers,\n",
    "            self.vision_self_attn_layers,\n",
    "            self.language_self_attn_layers\n",
    "        ):\n",
    "            vision_feats = vl_attn(vision_feats, text_feats, text_feats, text_mask)\n",
    "            text_feats = lv_attn(text_feats, vision_feats, vision_feats, vision_mask)\n",
    "\n",
    "            vision_feats = v_self(vision_feats, vision_feats, vision_feats, vision_mask)\n",
    "            text_feats = l_self(text_feats, text_feats, text_feats, text_mask)\n",
    "\n",
    "        fused_feats = torch.cat([vision_feats, text_feats], dim=1)  # shape: (B, V+T, D)\n",
    "\n",
    "        return fused_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:35:14.618738Z",
     "iopub.status.busy": "2025-04-18T13:35:14.618313Z",
     "iopub.status.idle": "2025-04-18T13:35:14.635794Z",
     "shell.execute_reply": "2025-04-18T13:35:14.635135Z",
     "shell.execute_reply.started": "2025-04-18T13:35:14.618720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NonTextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NonTextModel, self).__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.local_visual_extractor = EfficientNetFeatureExtractor(model_name='efficientnet_b7').to(self.device)\n",
    "        self.global_visual_extractor = Blip2ViTExtractor().to(self.device)\n",
    "        \n",
    "        self.bart_pho = BARTpho(device = self.device, use_lora = True)  # truy·ªÅn r√µ device\n",
    "        self.encoder = BiDirectionalCrossAttention().to(self.device)\n",
    "\n",
    "    def forward(self, images, questions, answers):\n",
    "        local_features = self.local_visual_extractor(images)\n",
    "        global_features = self.global_visual_extractor(images)\n",
    "        vision_feats = torch.cat([local_features, global_features], dim=1)\n",
    "        vision_feats = vision_feats.to(self.device)\n",
    "        vision_mask = torch.ones(vision_feats.size()[:-1], dtype=torch.bool).to(self.device)\n",
    "    \n",
    "        text_encoding = self.bart_pho.encode(questions)\n",
    "        text_feats = text_encoding[\"encoder_hidden_states\"]\n",
    "        question_attention_mask = text_encoding[\"attention_mask\"]\n",
    "    \n",
    "        encoder_output = self.encoder(vision_feats, vision_mask, text_feats, question_attention_mask)\n",
    "        encoder_attention_mask = torch.cat([vision_mask, question_attention_mask], dim=1)\n",
    "    \n",
    "        answer_encoded = self.bart_pho.tokenizer(\n",
    "            answers, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).to(self.device)\n",
    "        answer_input_ids = answer_encoded[\"input_ids\"]\n",
    "        answer_attention_mask = answer_encoded[\"attention_mask\"]\n",
    "    \n",
    "        logits = self.bart_pho.decode(\n",
    "            answer_input_ids=answer_input_ids,\n",
    "            answer_attention_mask=answer_attention_mask,\n",
    "            encoder_hidden_states=encoder_output,\n",
    "            encoder_attention_mask=encoder_attention_mask\n",
    "        )\n",
    "    \n",
    "        return logits\n",
    "\n",
    "    def generate(self, images, questions):\n",
    "         # Step 1: Extract visual features\n",
    "        with torch.no_grad():\n",
    "            local_features = self.local_visual_extractor(images)      # (B, N, D)\n",
    "            global_features = self.global_visual_extractor(images)    # (B, 1, D)\n",
    "            vision_feats = torch.cat([local_features, global_features], dim = 1)\n",
    "            vision_mask = torch.ones(vision_feats.size()[:-1], dtype = torch.bool).to(self.device)\n",
    "    \n",
    "            text_encoding = self.bart_pho.encode(questions)\n",
    "            text_feats = text_encoding[\"encoder_hidden_states\"]\n",
    "            question_attention_mask = text_encoding[\"attention_mask\"]\n",
    "    \n",
    "            # Step 3: Co-Attention Fusion\n",
    "            encoder_output = self.encoder(vision_feats, vision_mask, text_feats, question_attention_mask).to(self.device)\n",
    "            encoder_attention_mask = torch.cat([vision_mask, question_attention_mask], dim=1).to(self.device)\n",
    "    \n",
    "            return self.bart_pho.generate(encoder_output, encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:35:14.636533Z",
     "iopub.status.busy": "2025-04-18T13:35:14.636304Z",
     "iopub.status.idle": "2025-04-18T13:35:14.650836Z",
     "shell.execute_reply": "2025-04-18T13:35:14.650228Z",
     "shell.execute_reply.started": "2025-04-18T13:35:14.636512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VQATrainer:\n",
    "    def __init__(self, model, train_loader, criterion, optimizer, lr_scheduler):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Store training loss values for monitoring\n",
    "        self.train_losses = []\n",
    "\n",
    "    def train(self, start_epoch = 0, num_epochs = NUM_EPOCHS):\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(start_epoch, min(start_epoch + num_epochs, NUM_EPOCHS)):\n",
    "            total_loss = 0.0\n",
    "            pbar = tqdm(self.train_loader, desc=f\"[Epoch {epoch+1}/{NUM_EPOCHS}] Training\", leave=False)\n",
    "            numDatas = 0\n",
    "\n",
    "            for batch in pbar:\n",
    "                images, questions, answers = batch\n",
    "                # Forward pass\n",
    "                logits = self.model(images, questions, answers)\n",
    "\n",
    "                answer_encoded = self.model.bart_pho.tokenizer(\n",
    "                    answers, return_tensors=\"pt\", padding=True, truncation=True\n",
    "                )\n",
    "                answer_input_ids = answer_encoded[\"input_ids\"].to(self.device)\n",
    "                labels = answer_input_ids\n",
    "                \n",
    "                loss = self.criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix(loss = loss.item())\n",
    "                numDatas += 1\n",
    "\n",
    "            self.lr_scheduler.step()\n",
    "            avg_loss = total_loss / numDatas\n",
    "            print(f\"[Epoch {epoch + 1}] Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            #\n",
    "            self.train_losses.append(avg_loss)\n",
    "\n",
    "            #\n",
    "            self.save_checkpoint(epoch, CHECKPOINT_PATH)\n",
    "\n",
    "            api = HfApi()\n",
    "            api.upload_file(\n",
    "                path_or_fileobj = CHECKPOINT_PATH,                      # File tr√™n m√°y\n",
    "                path_in_repo = CHECKPOINT_FILENAME,              # File s·∫Ω n·∫±m trong th∆∞ m·ª•c m·ªõi\n",
    "                repo_id = HUGGINGFACE_HUB_REPO,                        # Repo ƒë√≠ch\n",
    "                repo_type = \"model\",                                      # Ho·∫∑c \"model\"\n",
    "                commit_message = f\"Completed training {BASELINE_NAME} until epoch {epoch + 1}!\"\n",
    "            )\n",
    "\n",
    "        print(\"Completed training!\")\n",
    "\n",
    "    def save_checkpoint(self, epoch, filepath):\n",
    "        # Save model checkpoint with necessary states\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.lr_scheduler.state_dict(),\n",
    "            'losses': self.train_losses\n",
    "        }\n",
    "        if os.path.exists(filepath):\n",
    "            os.remove(filepath)\n",
    "\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Checkpoint saved at {filepath}\")\n",
    "    \n",
    "    def load_checkpoint(self, filepath):\n",
    "        # Load checkpoint and restore model state\n",
    "        if os.path.isfile(filepath):\n",
    "            checkpoint = torch.load(filepath, map_location = self.device)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            self.train_losses = checkpoint['losses']\n",
    "            \n",
    "            print(f\"Loaded checkpoint from {filepath}, resuming from epoch {start_epoch + 1}\")\n",
    "            return start_epoch\n",
    "        else:\n",
    "            print(f\"No checkpoint found at {filepath}, starting from scratch\")\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:35:14.651819Z",
     "iopub.status.busy": "2025-04-18T13:35:14.651604Z",
     "iopub.status.idle": "2025-04-18T13:35:16.642009Z",
     "shell.execute_reply": "2025-04-18T13:35:16.641243Z",
     "shell.execute_reply.started": "2025-04-18T13:35:14.651802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_hf_dataset = load_dataset(\"Namronaldo2004/ViInfographicsVQA\", split = \"train\", streaming = True)\n",
    "train_dataset = ViInfographicsVQADataset(train_hf_dataset)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images, questions, answers = zip(*batch)\n",
    "\n",
    "    images = list(images)\n",
    "    questions = list(questions)\n",
    "    answers = list(answers)\n",
    "    \n",
    "    return images, questions, answers\n",
    "    \n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 16, shuffle = False, collate_fn = custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:35:16.643216Z",
     "iopub.status.busy": "2025-04-18T13:35:16.642901Z",
     "iopub.status.idle": "2025-04-18T13:36:14.392594Z",
     "shell.execute_reply": "2025-04-18T13:36:14.391852Z",
     "shell.execute_reply.started": "2025-04-18T13:35:16.643192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Kh·ªüi t·∫°o model\n",
    "VQA_model = NonTextModel()\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = VQA_model.bart_pho.tokenizer.pad_token_id)\n",
    "\n",
    "# Optimizer v√† scheduler\n",
    "optimizer = optim.AdamW(VQA_model.parameters(), lr = 1e-8)\n",
    "lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.9)\n",
    "\n",
    "# Kh·ªüi t·∫°o class Trainer\n",
    "VQA_trainer = VQATrainer(VQA_model, train_dataloader, criterion, optimizer, lr_scheduler)\n",
    "start_epoch = VQA_trainer.load_checkpoint(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:36:14.393812Z",
     "iopub.status.busy": "2025-04-18T13:36:14.393558Z",
     "iopub.status.idle": "2025-04-18T13:36:21.636561Z",
     "shell.execute_reply": "2025-04-18T13:36:21.635912Z",
     "shell.execute_reply.started": "2025-04-18T13:36:14.393794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample = next(iter(train_hf_dataset))\n",
    "image = sample[\"image\"]\n",
    "question = sample[\"question\"]\n",
    "\n",
    "VQA_model.generate([image], [question])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:36:21.637484Z",
     "iopub.status.busy": "2025-04-18T13:36:21.637212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "VQA_trainer.train(start_epoch = start_epoch, num_epochs = 2)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
