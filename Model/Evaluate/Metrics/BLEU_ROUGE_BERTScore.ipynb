{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa1dc90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T02:20:32.964782Z",
     "iopub.status.busy": "2025-04-30T02:20:32.964308Z",
     "iopub.status.idle": "2025-04-30T02:20:39.719687Z",
     "shell.execute_reply": "2025-04-30T02:20:39.718376Z",
     "shell.execute_reply.started": "2025-04-30T02:20:32.964751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk rouge-score bert-score numpy torch transformers underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31088c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T02:20:39.986938Z",
     "iopub.status.busy": "2025-04-30T02:20:39.986689Z",
     "iopub.status.idle": "2025-04-30T02:20:39.999527Z",
     "shell.execute_reply": "2025-04-30T02:20:39.998435Z",
     "shell.execute_reply.started": "2025-04-30T02:20:39.986919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from underthesea import word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_vqa_metrics(references: List[str], hypotheses: List[str], bert_batch_size: int = 16) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate BLEU, ROUGE-1, ROUGE-2, ROUGE-L, and BERTScore metrics for VQA model evaluation in Vietnamese.\n",
    "    \n",
    "    Args:\n",
    "        references: List of reference answers (ground truth) for each question in Vietnamese.\n",
    "        hypotheses: List of predicted answers from the VQA model in Vietnamese.\n",
    "        bert_batch_size: Batch size for BERTScore computation (default 512).\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing BLEU, ROUGE-1, ROUGE-2, ROUGE-L, and BERTScore metrics.\n",
    "    \"\"\"\n",
    "    if len(references) != len(hypotheses):\n",
    "        raise ValueError(\"Number of references and hypotheses must match.\")\n",
    "    if not references or not hypotheses:\n",
    "        raise ValueError(\"Input lists cannot be empty.\")\n",
    "    \n",
    "    bleu_scores = []\n",
    "    rouge_1_scores = []\n",
    "    rouge_2_scores = []\n",
    "    rouge_l_scores = []\n",
    "    bert_p_scores = []\n",
    "    bert_r_scores = []\n",
    "    bert_f1_scores = []\n",
    "\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "\n",
    "    # Calculate BLEU and ROUGE\n",
    "    for ref, hyp in tqdm(zip(references, hypotheses), desc = \"Calculating BLEU & ROUGE\", total=len(references)):\n",
    "        ref_tokens = [word_tokenize(ref.lower(), format = \"text\").split()]\n",
    "        hyp_tokens = word_tokenize(hyp.lower(), format = \"text\").split()\n",
    "        \n",
    "        try:\n",
    "            bleu = sentence_bleu(ref_tokens, hyp_tokens, \n",
    "                                 smoothing_function=SmoothingFunction().method1)\n",
    "        except ZeroDivisionError:\n",
    "            bleu = 0.0\n",
    "        bleu_scores.append(bleu)\n",
    "        \n",
    "        rouge_scores = rouge.score(ref, hyp)\n",
    "        rouge_1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge_2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rouge_l_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "\n",
    "    # Calculate BERTScore in batches with tqdm\n",
    "    print(\"Calculating BERTScore...\")\n",
    "    try:\n",
    "        for i in tqdm(range(0, len(hypotheses), bert_batch_size), desc=\"BERTScore batches\"):\n",
    "            batch_hyps = hypotheses[i:i+bert_batch_size]\n",
    "            batch_refs = references[i:i+bert_batch_size]\n",
    "            P, R, F1 = score(batch_hyps, batch_refs, \n",
    "                             lang=\"vi\", model_type=\"xlm-roberta-base\", verbose=False)\n",
    "            bert_p_scores.extend(P.tolist())\n",
    "            bert_r_scores.extend(R.tolist())\n",
    "            bert_f1_scores.extend(F1.tolist())\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: BERTScore calculation failed: {e}\")\n",
    "        bert_p_scores = [0.0] * len(hypotheses)\n",
    "        bert_r_scores = [0.0] * len(hypotheses)\n",
    "        bert_f1_scores = [0.0] * len(hypotheses)\n",
    "    \n",
    "    # Compute average scores\n",
    "    metrics = {\n",
    "        'BLEU': float(np.mean(bleu_scores)),\n",
    "        'ROUGE-1': float(np.mean(rouge_1_scores)),\n",
    "        'ROUGE-2': float(np.mean(rouge_2_scores)),\n",
    "        'ROUGE-L': float(np.mean(rouge_l_scores)),\n",
    "        'BERTScore_Precision': float(np.mean(bert_p_scores)),\n",
    "        'BERTScore_Recall': float(np.mean(bert_r_scores)),\n",
    "        'BERTScore_F1': float(np.mean(bert_f1_scores))\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f6fdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T02:20:40.001408Z",
     "iopub.status.busy": "2025-04-30T02:20:40.001040Z",
     "iopub.status.idle": "2025-04-30T02:21:22.347058Z",
     "shell.execute_reply": "2025-04-30T02:21:22.345832Z",
     "shell.execute_reply.started": "2025-04-30T02:20:40.001377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data in Vietnamese\n",
    "    refs = [\n",
    "        \"Con mèo nằm trên thảm.\",\n",
    "        \"Bầu trời màu xanh.\"\n",
    "    ]\n",
    "    hyps = [\n",
    "        \"Con mèo nằm trên thảm.\",\n",
    "        \"Trời màu xanh.\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        results = calculate_vqa_metrics(refs, hyps)\n",
    "        for metric, value in results.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in metric calculation: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
