{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:44:54.553933Z",
     "iopub.status.busy": "2025-04-29T16:44:54.553384Z",
     "iopub.status.idle": "2025-04-29T16:44:54.560364Z",
     "shell.execute_reply": "2025-04-29T16:44:54.559571Z",
     "shell.execute_reply.started": "2025-04-29T16:44:54.553908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:44:54.561535Z",
     "iopub.status.busy": "2025-04-29T16:44:54.561347Z",
     "iopub.status.idle": "2025-04-29T16:46:44.340217Z",
     "shell.execute_reply": "2025-04-29T16:46:44.339464Z",
     "shell.execute_reply.started": "2025-04-29T16:44:54.561520Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch \n",
    "!pip install torchvision\n",
    "!pip install transformers\n",
    "!pip install peft==0.10.0\n",
    "!pip install datasets\n",
    "!pip install pillow\n",
    "!pip install underthesea\n",
    "!pip install huggingface_hub\n",
    "!pip install hf_xet\n",
    "!pip install bert-score\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-29T16:46:44.341827Z",
     "iopub.status.busy": "2025-04-29T16:46:44.341590Z",
     "iopub.status.idle": "2025-04-29T16:47:09.600739Z",
     "shell.execute_reply": "2025-04-29T16:47:09.600031Z",
     "shell.execute_reply.started": "2025-04-29T16:46:44.341803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "from PIL import ImageFile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import hf_hub_download, HfApi\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:47:09.602216Z",
     "iopub.status.busy": "2025-04-29T16:47:09.601598Z",
     "iopub.status.idle": "2025-04-29T16:47:09.606152Z",
     "shell.execute_reply": "2025-04-29T16:47:09.605610Z",
     "shell.execute_reply.started": "2025-04-29T16:47:09.602191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define project-specific variables\n",
    "PROJECT_NAME = \"ViInfographicsVQA\"  # Name of the project\n",
    "USERNAME = \"Namronaldo2004\"          # Hugging Face username\n",
    "HUGGINGFACE_HUB_REPO = USERNAME + \"/\" + PROJECT_NAME  # Full repository name on Hugging Face Hub\n",
    "BASELINE_NAME = \"Flow3-modified/Text\"\n",
    "CHECKPOINT_FILENAME = f\"{BASELINE_NAME}/latest_checkpoint.pth\"\n",
    "OCR_VAL_FILENAME = \"final_val_OCR.txt\"\n",
    "OCR_TEST_FILENAME = \"final_test_OCR.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:47:09.607841Z",
     "iopub.status.busy": "2025-04-29T16:47:09.607630Z",
     "iopub.status.idle": "2025-04-29T16:47:30.843918Z",
     "shell.execute_reply": "2025-04-29T16:47:30.843320Z",
     "shell.execute_reply.started": "2025-04-29T16:47:09.607826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "CHECKPOINT_PATH = \"./checkpoints/latest_checkpoint.pth\"\n",
    "if api.file_exists(\n",
    "    repo_id = HUGGINGFACE_HUB_REPO, \n",
    "    filename = CHECKPOINT_FILENAME, \n",
    "    repo_type = \"model\"\n",
    "):\n",
    "    CHECKPOINT_PATH = hf_hub_download(\n",
    "        repo_id = HUGGINGFACE_HUB_REPO, \n",
    "        filename = CHECKPOINT_FILENAME, \n",
    "        local_dir = \"./checkpoints\",  # Store the checkpoint locally in the \"checkpoints\" directory\n",
    "        local_dir_use_symlinks = False  # Avoid using symlinks for compatibility\n",
    "    )\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok = True)\n",
    "print(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:47:30.844819Z",
     "iopub.status.busy": "2025-04-29T16:47:30.844579Z",
     "iopub.status.idle": "2025-04-29T16:47:30.850281Z",
     "shell.execute_reply": "2025-04-29T16:47:30.849586Z",
     "shell.execute_reply.started": "2025-04-29T16:47:30.844791Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ViInfographicsVQATextDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for streaming text-image pairs from a Hugging Face dataset.\n",
    "    This dataset yields image-text pairs one by one, without preloading everything into memory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset, ocr_contents, max_instances = None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            hf_dataset: The Hugging Face dataset containing image-text pairs.\n",
    "            transform: Optional image transformations (e.g., resizing, normalization).\n",
    "            max_instances: Maximum number of instances to process.\n",
    "        \"\"\"\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.ocr_contents = ocr_contents\n",
    "        self.max_instances = max_instances\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterator to stream dataset samples.\n",
    "        \n",
    "        Yields:\n",
    "            A tuple (image, text) where:\n",
    "            - image: Transformed image tensor\n",
    "            - text: Corresponding textual prompt\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        temp_count = 0\n",
    "        for sample in self.hf_dataset:\n",
    "            if (self.max_instances is not None and count >= self.max_instances):\n",
    "                break  # Stop if max instances reached\n",
    "\n",
    "            question_type = sample[\"type\"]\n",
    "            if (question_type.lower() == \"text\"):\n",
    "                ocr_info = self.ocr_contents[str(temp_count)]\n",
    "                question = sample[\"question\"]\n",
    "                answer = sample[\"answer\"]\n",
    "                \n",
    "                yield ocr_info, question, answer\n",
    "                count += 1  # Increment the counter\n",
    "\n",
    "            temp_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:47:30.851224Z",
     "iopub.status.busy": "2025-04-29T16:47:30.851015Z",
     "iopub.status.idle": "2025-04-29T16:47:30.882715Z",
     "shell.execute_reply": "2025-04-29T16:47:30.882074Z",
     "shell.execute_reply.started": "2025-04-29T16:47:30.851203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BARTPho(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"vinai/bartpho-syllable\",\n",
    "        device=\"cpu\",\n",
    "        max_length=50,\n",
    "        use_lora=False,\n",
    "        lora_r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Load base model\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "        # Apply LoRA if needed\n",
    "        if use_lora:\n",
    "            lora_config = LoraConfig(\n",
    "                r=lora_r,\n",
    "                lora_alpha=lora_alpha,\n",
    "                lora_dropout=lora_dropout,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                target_modules=target_modules\n",
    "            )\n",
    "            base_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "        # Save full model\n",
    "        self.encoder = base_model.base_model.model.model.encoder.to(device)\n",
    "        self.decoder = base_model.base_model.model.model.decoder  # BART/M-BART decoder\n",
    "        self.lm_head = base_model.base_model.model.lm_head\n",
    "\n",
    "    def encode(self, input_texts):\n",
    "        batch_encoder_hidden_states = []\n",
    "        batch_attention_masks = []\n",
    "        batch_input_ids = []\n",
    "    \n",
    "        for text in input_texts:\n",
    "            # Split sentence into chunks (each chunk has at most 512 words)\n",
    "            words = text.split()\n",
    "            chunks = [' '.join(words[i * 384: (i + 1) * 384]) for i in range((len(words) + 383) // 384)]\n",
    "    \n",
    "            chunk_input_ids_list = []\n",
    "            chunk_attention_mask_list = []\n",
    "            chunk_hidden_states_list = []\n",
    "    \n",
    "            for chunk in chunks:\n",
    "                encoded = self.tokenizer(\n",
    "                    chunk,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True\n",
    "                ).to(self.device)\n",
    "    \n",
    "                input_ids = encoded[\"input_ids\"]           # shape: (1, seq_len)\n",
    "                attention_mask = encoded[\"attention_mask\"] # shape: (1, seq_len)\n",
    "    \n",
    "                outputs = self.encoder(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "    \n",
    "                chunk_input_ids_list.append(input_ids)\n",
    "                chunk_attention_mask_list.append(attention_mask)\n",
    "                chunk_hidden_states_list.append(outputs.last_hidden_state)\n",
    "    \n",
    "            # Concatenate all chunks → shape: (1, total_seq_len, hidden_size)\n",
    "            full_input_ids = torch.cat(chunk_input_ids_list, dim=1).squeeze(0)          # shape: (total_seq_len)\n",
    "            full_attention_mask = torch.cat(chunk_attention_mask_list, dim=1).squeeze(0)# shape: (total_seq_len)\n",
    "            full_hidden_states = torch.cat(chunk_hidden_states_list, dim=1).squeeze(0)  # shape: (total_seq_len, hidden_size)\n",
    "    \n",
    "            batch_input_ids.append(full_input_ids)\n",
    "            batch_attention_masks.append(full_attention_mask)\n",
    "            batch_encoder_hidden_states.append(full_hidden_states)\n",
    "    \n",
    "        # Pad sequences in the batch to the same length\n",
    "        max_seq_len = max(x.shape[0] for x in batch_input_ids)\n",
    "    \n",
    "        def pad_tensor(tensor, max_len, pad_value=0):\n",
    "            pad_len = max_len - tensor.shape[0]\n",
    "            if pad_len == 0:\n",
    "                return tensor\n",
    "            pad = torch.full((pad_len,), pad_value, dtype=tensor.dtype, device=tensor.device)\n",
    "            return torch.cat([tensor, pad], dim=0)\n",
    "    \n",
    "        def pad_hidden_states(tensor, max_len):\n",
    "            pad_len = max_len - tensor.shape[0]\n",
    "            if pad_len == 0:\n",
    "                return tensor\n",
    "            pad = torch.zeros((pad_len, tensor.shape[1]), device=tensor.device)\n",
    "            return torch.cat([tensor, pad], dim=0)\n",
    "    \n",
    "        # Stack padded tensors\n",
    "        input_ids = torch.stack([pad_tensor(x, max_seq_len, self.tokenizer.pad_token_id) for x in batch_input_ids], dim=0)\n",
    "        attention_mask = torch.stack([pad_tensor(x, max_seq_len, 0) for x in batch_attention_masks], dim=0)\n",
    "        encoder_hidden_states = torch.stack([pad_hidden_states(x, max_seq_len) for x in batch_encoder_hidden_states], dim=0)\n",
    "    \n",
    "        return {\n",
    "            \"encoder_hidden_states\": encoder_hidden_states,  # shape: (batch_size, seq_len, hidden_size)\n",
    "            \"attention_mask\": attention_mask,                # shape: (batch_size, seq_len)\n",
    "            \"input_ids\": input_ids                           # shape: (batch_size, seq_len)\n",
    "        }\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        answer_input_ids,\n",
    "        answer_attention_mask,\n",
    "        encoder_hidden_states,\n",
    "        encoder_attention_mask,\n",
    "    ):    \n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=answer_input_ids,\n",
    "            attention_mask=answer_attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "        )\n",
    "        \n",
    "        logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, encoder_hidden_states, encoder_attention_mask):\n",
    "        batch_size = encoder_hidden_states.size(0)\n",
    "        device = encoder_hidden_states.device\n",
    "    \n",
    "        # Bắt đầu với decoder_input_ids là eos_token_id cho mỗi dòng trong batch\n",
    "        decoder_input_ids = torch.full(\n",
    "            (batch_size, 1),\n",
    "            fill_value=self.tokenizer.eos_token_id,\n",
    "            dtype=torch.long,\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for _ in range(self.max_length):\n",
    "                logits = self.decode(\n",
    "                    answer_input_ids=decoder_input_ids,\n",
    "                    answer_attention_mask=torch.ones_like(decoder_input_ids, device=device),\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask\n",
    "                )\n",
    "    \n",
    "                next_token = logits[:, -1, :].argmax(-1, keepdim=True)  # (B, 1)\n",
    "                decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=-1)\n",
    "    \n",
    "                # Dừng sớm nếu tất cả dòng đều sinh ra <eos>\n",
    "                if (next_token == self.tokenizer.eos_token_id).all():\n",
    "                    break\n",
    "    \n",
    "        return self.tokenizer.batch_decode(decoder_input_ids, skip_special_tokens = True)\n",
    "\n",
    "    def freeze_encoder(self, layers_to_freeze=None):\n",
    "        if layers_to_freeze is None:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for idx, layer in enumerate(self.encoder.layers):\n",
    "                if idx in layers_to_freeze:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def unfreeze_encoder(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:47:30.883574Z",
     "iopub.status.busy": "2025-04-29T16:47:30.883368Z",
     "iopub.status.idle": "2025-04-29T16:47:30.899744Z",
     "shell.execute_reply": "2025-04-29T16:47:30.899071Z",
     "shell.execute_reply.started": "2025-04-29T16:47:30.883554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model = 768, d_ff = 2048, dropout = 0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dropout1(F.gelu(self.fc1(x)))\n",
    "        out = self.dropout2(self.fc2(out))\n",
    "        return self.norm(x + out)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model = 768, num_heads = 8, dropout = 0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim = d_model, num_heads = num_heads, dropout = dropout, batch_first = True)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask=None):\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = ~attention_mask.bool()  # Convert to padding mask: True = MASK\n",
    "        attn_output, _ = self.attn(queries, keys, values, key_padding_mask = attention_mask)\n",
    "        out = self.dropout(attn_output)\n",
    "        return self.norm(queries + out)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model = 768, num_heads = 8, d_ff = 2048, dropout = 0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mhatt = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.pwff = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask = None):\n",
    "        att = self.mhatt(queries, keys, values, attention_mask)\n",
    "        ff = self.pwff(att)\n",
    "        return ff\n",
    "\n",
    "class BiDirectionalCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model = 1024, num_heads = 8, d_ff = 2048, dropout = 0.1, num_layers = 3, max_len = 6000):\n",
    "        super(BiDirectionalCrossAttention, self).__init__()\n",
    "\n",
    "        self.vision_pos_embed = nn.Embedding(max_len, d_model)\n",
    "        self.text_pos_embed = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        self.vision_norm = nn.LayerNorm(d_model)\n",
    "        self.text_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.d_model = d_model  # D = 1024\n",
    "\n",
    "        self.vision_language_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.language_vision_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.vision_self_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.language_self_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, vision_feats, vision_mask, text_feats, text_mask):\n",
    "        batch_size, v_len, _ = vision_feats.size()\n",
    "        _, t_len, _ = text_feats.size()\n",
    "\n",
    "        v_pos_ids = torch.arange(v_len, device=vision_feats.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        t_pos_ids = torch.arange(t_len, device=text_feats.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "        vision_feats = self.vision_norm(vision_feats + self.vision_pos_embed(v_pos_ids))\n",
    "        text_feats = self.text_norm(text_feats + self.text_pos_embed(t_pos_ids))\n",
    "\n",
    "        for vl_attn, lv_attn, v_self, l_self in zip(\n",
    "            self.vision_language_attn_layers,\n",
    "            self.language_vision_attn_layers,\n",
    "            self.vision_self_attn_layers,\n",
    "            self.language_self_attn_layers\n",
    "        ):\n",
    "            vision_feats = vl_attn(vision_feats, text_feats, text_feats, text_mask)\n",
    "            text_feats = lv_attn(text_feats, vision_feats, vision_feats, vision_mask)\n",
    "\n",
    "            vision_feats = v_self(vision_feats, vision_feats, vision_feats, vision_mask)\n",
    "            text_feats = l_self(text_feats, text_feats, text_feats, text_mask)\n",
    "\n",
    "        fused_feats = torch.cat([vision_feats, text_feats], dim=1)  # shape: (B, V+T, D)\n",
    "\n",
    "        return fused_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:47:30.900877Z",
     "iopub.status.busy": "2025-04-29T16:47:30.900642Z",
     "iopub.status.idle": "2025-04-29T16:47:30.918241Z",
     "shell.execute_reply": "2025-04-29T16:47:30.917568Z",
     "shell.execute_reply.started": "2025-04-29T16:47:30.900863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextModel, self).__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.bart_pho = BARTPho(device = self.device, use_lora = True)  # truyền rõ device\n",
    "        self.bart_pho.freeze_encoder()\n",
    "        self.encoder = BiDirectionalCrossAttention().to(self.device)\n",
    "\n",
    "    def forward(self, ocr_infos, questions, answers):\n",
    "        ocr_encoding = self.bart_pho.encode(ocr_infos)\n",
    "        ocr_feats = ocr_encoding[\"encoder_hidden_states\"]\n",
    "        ocr_attention_mask = ocr_encoding[\"attention_mask\"]\n",
    "    \n",
    "        question_encoding = self.bart_pho.encode(questions)\n",
    "        question_feats = question_encoding[\"encoder_hidden_states\"]\n",
    "        question_attention_mask = question_encoding[\"attention_mask\"]\n",
    "    \n",
    "        encoder_output = self.encoder(ocr_feats, ocr_attention_mask, question_feats, question_attention_mask)\n",
    "        encoder_attention_mask = torch.cat([ocr_attention_mask, question_attention_mask], dim = 1)\n",
    "    \n",
    "        answer_encoded = self.bart_pho.tokenizer(\n",
    "            answers, return_tensors = \"pt\", padding = True, truncation = True\n",
    "        ).to(self.device)\n",
    "        answer_input_ids = answer_encoded[\"input_ids\"]\n",
    "        answer_attention_mask = answer_encoded[\"attention_mask\"]\n",
    "    \n",
    "        logits = self.bart_pho.decode(\n",
    "            answer_input_ids = answer_input_ids,\n",
    "            answer_attention_mask = answer_attention_mask,\n",
    "            encoder_hidden_states = encoder_output,\n",
    "            encoder_attention_mask = encoder_attention_mask\n",
    "        )\n",
    "    \n",
    "        return logits\n",
    "\n",
    "    def generate(self, ocr_infos, questions):\n",
    "         # Step 1: Extract visual features\n",
    "        with torch.no_grad():\n",
    "            ocr_encoding = self.bart_pho.encode(ocr_infos)\n",
    "            ocr_feats = ocr_encoding[\"encoder_hidden_states\"]\n",
    "            ocr_attention_mask = ocr_encoding[\"attention_mask\"]\n",
    "        \n",
    "            question_encoding = self.bart_pho.encode(questions)\n",
    "            question_feats = question_encoding[\"encoder_hidden_states\"]\n",
    "            question_attention_mask = question_encoding[\"attention_mask\"]\n",
    "        \n",
    "            encoder_output = self.encoder(ocr_feats, ocr_attention_mask, question_feats, question_attention_mask)\n",
    "            encoder_attention_mask = torch.cat([ocr_attention_mask, question_attention_mask], dim = 1)\n",
    "    \n",
    "            return self.bart_pho.generate(encoder_output, encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:47:33.972315Z",
     "iopub.status.busy": "2025-04-29T16:47:33.972084Z",
     "iopub.status.idle": "2025-04-29T16:47:34.608054Z",
     "shell.execute_reply": "2025-04-29T16:47:34.607317Z",
     "shell.execute_reply.started": "2025-04-29T16:47:33.972300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "OCR_VAL_CHECKPOINT_PATH = hf_hub_download(\n",
    "    repo_id = HUGGINGFACE_HUB_REPO, \n",
    "    filename = OCR_VAL_FILENAME, \n",
    "    local_dir = \"./checkpoints\",  # Store the checkpoint locally in the \"checkpoints\" directory\n",
    "    local_dir_use_symlinks = False  # Avoid using symlinks for compatibility\n",
    ")\n",
    "\n",
    "print(OCR_VAL_CHECKPOINT_PATH)\n",
    "\n",
    "val_ocr_contents = dict()\n",
    "with open (OCR_VAL_CHECKPOINT_PATH, 'r', encoding = 'utf-8') as ocr_file:\n",
    "    lines = ocr_file.readlines()\n",
    "    for line in lines:\n",
    "        temp = line.split(\":\")\n",
    "        ocr_index = int(temp[0].split(\" \")[1])\n",
    "        ocr_content = \":\".join(temp[1:]).strip()\n",
    "        val_ocr_contents[str(ocr_index)] = ocr_content\n",
    "\n",
    "val_ocr_contents = dict(sorted(val_ocr_contents.items(), key = lambda x: int(x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:47:34.609150Z",
     "iopub.status.busy": "2025-04-29T16:47:34.608930Z",
     "iopub.status.idle": "2025-04-29T16:47:35.644359Z",
     "shell.execute_reply": "2025-04-29T16:47:35.643816Z",
     "shell.execute_reply.started": "2025-04-29T16:47:34.609123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "OCR_TEST_CHECKPOINT_PATH = hf_hub_download(\n",
    "    repo_id = HUGGINGFACE_HUB_REPO, \n",
    "    filename = OCR_TEST_FILENAME, \n",
    "    local_dir = \"./checkpoints\",  # Store the checkpoint locally in the \"checkpoints\" directory\n",
    "    local_dir_use_symlinks = False  # Avoid using symlinks for compatibility\n",
    ")\n",
    "\n",
    "print(OCR_TEST_CHECKPOINT_PATH)\n",
    "\n",
    "test_ocr_contents = dict()\n",
    "with open (OCR_TEST_CHECKPOINT_PATH, 'r', encoding = 'utf-8') as ocr_file:\n",
    "    lines = ocr_file.readlines()\n",
    "    for line in lines:\n",
    "        temp = line.split(\":\")\n",
    "        ocr_index = int(temp[0].split(\" \")[1])\n",
    "        ocr_content = \":\".join(temp[1:]).strip()\n",
    "        test_ocr_contents[str(ocr_index)] = ocr_content\n",
    "\n",
    "test_ocr_contents = dict(sorted(test_ocr_contents.items(), key = lambda x: int(x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:47:35.645700Z",
     "iopub.status.busy": "2025-04-29T16:47:35.645154Z",
     "iopub.status.idle": "2025-04-29T16:47:39.426763Z",
     "shell.execute_reply": "2025-04-29T16:47:39.426092Z",
     "shell.execute_reply.started": "2025-04-29T16:47:35.645673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_hf_dataset = load_dataset(\"Namronaldo2004/ViInfographicsVQA\", split = \"val\", streaming = True)\n",
    "test_hf_dataset = load_dataset(\"Namronaldo2004/ViInfographicsVQA\", split = \"test\", streaming = True)\n",
    "val_dataset = ViInfographicsVQATextDataset(val_hf_dataset, val_ocr_contents)\n",
    "test_dataset = ViInfographicsVQATextDataset(test_hf_dataset, test_ocr_contents)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    ocr_infos, questions, answers = zip(*batch)\n",
    "    \n",
    "    ocr_infos = list(ocr_infos)\n",
    "    questions = list(questions)\n",
    "    answers = list(answers)\n",
    "    \n",
    "    return ocr_infos, questions, answers\n",
    "    \n",
    "val_dataloader = DataLoader(val_dataset, batch_size = 2, shuffle = False, collate_fn = custom_collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 2, shuffle = False, collate_fn = custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:47:39.427825Z",
     "iopub.status.busy": "2025-04-29T16:47:39.427558Z",
     "iopub.status.idle": "2025-04-29T16:47:59.079974Z",
     "shell.execute_reply": "2025-04-29T16:47:59.079019Z",
     "shell.execute_reply.started": "2025-04-29T16:47:39.427801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_checkpoint(model, filepath):\n",
    "    # Load checkpoint and restore model state\n",
    "    if os.path.isfile(filepath):\n",
    "        checkpoint = torch.load(filepath, map_location = DEVICE)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Khởi tạo model\n",
    "VQA_model = TextModel().to(DEVICE)\n",
    "load_checkpoint(VQA_model, CHECKPOINT_PATH)\n",
    "VQA_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:59:31.006114Z",
     "iopub.status.busy": "2025-04-29T16:59:31.005324Z",
     "iopub.status.idle": "2025-04-29T16:59:38.006576Z",
     "shell.execute_reply": "2025-04-29T16:59:38.005903Z",
     "shell.execute_reply.started": "2025-04-29T16:59:31.006079Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample = next(iter(val_hf_dataset))\n",
    "ocr_info = val_ocr_contents[\"0\"]\n",
    "question = sample[\"question\"]\n",
    "\n",
    "VQA_model.generate([ocr_info], [question])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:47:59.081665Z",
     "iopub.status.busy": "2025-04-29T16:47:59.081380Z",
     "iopub.status.idle": "2025-04-29T16:50:32.630572Z",
     "shell.execute_reply": "2025-04-29T16:50:32.629692Z",
     "shell.execute_reply.started": "2025-04-29T16:47:59.081646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_and_save_answers(dataloader, output_file_path, dataset_name = \"\"):\n",
    "    with open(output_file_path, 'w', encoding = 'utf-8') as f_out:\n",
    "        for batch in tqdm(dataloader, desc = f\"Generating answers for {dataset_name}\"):\n",
    "            ocr_infos, questions, _ = batch  # ignore groundtruth answers\n",
    "\n",
    "            # Inference (generate returns list of decoded strings)\n",
    "            with torch.no_grad():\n",
    "                generated_answers = VQA_model.generate(ocr_infos, questions)\n",
    "\n",
    "            # Save each generated answer to file\n",
    "            for answer in generated_answers:\n",
    "                f_out.write(answer.strip() + '\\n')\n",
    "\n",
    "# Generate and save for val and test\n",
    "generate_and_save_answers(val_dataloader, \"val_answer_flow3_text.txt\", \"val\")\n",
    "generate_and_save_answers(test_dataloader, \"test_answer_flow3_text.txt\", \"test\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
