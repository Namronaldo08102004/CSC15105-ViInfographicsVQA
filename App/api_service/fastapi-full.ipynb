{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4151008e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:15:48.462260Z",
     "iopub.status.busy": "2025-05-13T04:15:48.461992Z",
     "iopub.status.idle": "2025-05-13T04:18:10.989986Z",
     "shell.execute_reply": "2025-05-13T04:18:10.989078Z",
     "shell.execute_reply.started": "2025-05-13T04:15:48.462227Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install salesforce-lavis\n",
    "!pip install torch \n",
    "!pip install torchvision\n",
    "!pip install transformers\n",
    "!pip install peft==0.10.0\n",
    "!pip install datasets\n",
    "!pip install pillow\n",
    "!pip install underthesea\n",
    "!pip install huggingface_hub\n",
    "!pip install hf_xet\n",
    "!pip install google-genai\n",
    "!pip install easyocr\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c90aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:10.991297Z",
     "iopub.status.busy": "2025-05-13T04:18:10.991028Z",
     "iopub.status.idle": "2025-05-13T04:18:37.930840Z",
     "shell.execute_reply": "2025-05-13T04:18:37.930284Z",
     "shell.execute_reply.started": "2025-05-13T04:18:10.991269Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 04:18:19.240760: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747109899.467328      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747109899.534744      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/usr/local/lib/python3.11/dist-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
      "/usr/local/lib/python3.11/dist-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from lavis.models import load_model_and_preprocess\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import hf_hub_download, HfApi\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import GenerateContentConfig\n",
    "import easyocr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6242e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:37.932018Z",
     "iopub.status.busy": "2025-05-13T04:18:37.931554Z",
     "iopub.status.idle": "2025-05-13T04:18:37.936164Z",
     "shell.execute_reply": "2025-05-13T04:18:37.935241Z",
     "shell.execute_reply.started": "2025-05-13T04:18:37.931994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define project-specific variables\n",
    "PROJECT_NAME = \"ViInfographicsVQA\"  # Name of the project\n",
    "USERNAME = \"Namronaldo2004\"  # Hugging Face username\n",
    "HUGGINGFACE_HUB_REPO = (\n",
    "    USERNAME + \"/\" + PROJECT_NAME\n",
    ")  # Full repository name on Hugging Face Hub\n",
    "TEXT_BASELINE_NAME = \"Flow3-modified/Text\"\n",
    "TEXT_CHECKPOINT_FILENAME = f\"{TEXT_BASELINE_NAME}/latest_checkpoint.pth\"\n",
    "NONTEXT_BASELINE_NAME = \"Flow3-modified/Non-text\"\n",
    "NONTEXT_CHECKPOINT_FILENAME = f\"{NONTEXT_BASELINE_NAME}/latest_checkpoint.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8021d4eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:37.937653Z",
     "iopub.status.busy": "2025-05-13T04:18:37.937052Z",
     "iopub.status.idle": "2025-05-13T04:18:38.139676Z",
     "shell.execute_reply": "2025-05-13T04:18:38.138772Z",
     "shell.execute_reply.started": "2025-05-13T04:18:37.937625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "OCR_READER = easyocr.Reader([\"vi\"])\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "CLIENT = genai.Client(api_key=GEMINI_API_KEY)\n",
    "GEMINI_MODEL = \"gemini-2.5-flash-preview-04-17\"\n",
    "GENERATION_CONFIG = GenerateContentConfig(response_mime_type=\"text/plain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057495d4-cbdb-424d-98d5-f99c92ab326f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:38.140812Z",
     "iopub.status.busy": "2025-05-13T04:18:38.140453Z",
     "iopub.status.idle": "2025-05-13T04:18:59.443717Z",
     "shell.execute_reply": "2025-05-13T04:18:59.442874Z",
     "shell.execute_reply.started": "2025-05-13T04:18:38.140792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/Flow3-modified/Non-text/latest_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "api = HfApi()\n",
    "NONTEXT_CHECKPOINT_PATH = \"./checkpoints/nontext_latest_checkpoint.pth\"\n",
    "TEXT_CHECKPOINT_PATH = \"./checkpoints/text_latest_checkpoint.pth\"\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "if not os.path.exists(NONTEXT_CHECKPOINT_PATH):\n",
    "    if api.file_exists(\n",
    "        repo_id=HUGGINGFACE_HUB_REPO,\n",
    "        filename=NONTEXT_CHECKPOINT_FILENAME,\n",
    "        repo_type=\"model\",\n",
    "    ):\n",
    "        NONTEXT_CHECKPOINT_PATH = hf_hub_download(\n",
    "            repo_id=HUGGINGFACE_HUB_REPO,\n",
    "            filename=NONTEXT_CHECKPOINT_FILENAME,\n",
    "            local_dir=\"./checkpoints\",  # Store the checkpoint locally in the \"checkpoints\" directory\n",
    "        )\n",
    "print(NONTEXT_CHECKPOINT_PATH)\n",
    "# =======================================================================================================\n",
    "if not os.path.exists(TEXT_CHECKPOINT_PATH):\n",
    "    if api.file_exists(\n",
    "        repo_id=HUGGINGFACE_HUB_REPO,\n",
    "        filename=TEXT_CHECKPOINT_FILENAME,\n",
    "        repo_type=\"model\",\n",
    "    ):\n",
    "        TEXT_CHECKPOINT_PATH = hf_hub_download(\n",
    "            repo_id=HUGGINGFACE_HUB_REPO,\n",
    "            filename=TEXT_CHECKPOINT_FILENAME,\n",
    "            local_dir=\"./checkpoints\",  # Store the checkpoint locally in the \"checkpoints\" directory\n",
    "        )\n",
    "print(TEXT_CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d23e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.446459Z",
     "iopub.status.busy": "2025-05-13T04:18:59.446233Z",
     "iopub.status.idle": "2025-05-13T04:18:59.459022Z",
     "shell.execute_reply": "2025-05-13T04:18:59.458141Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.446438Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EfficientNetFeatureExtractor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"efficientnet_b0\",\n",
    "        target_size: int = 224,\n",
    "        central_fraction: float = 0.875,\n",
    "    ):\n",
    "        super(EfficientNetFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self._load_model(model_name).to(self.device)\n",
    "        self.transform = self._build_transform(target_size, central_fraction)\n",
    "\n",
    "        self.pooling1 = nn.AdaptiveAvgPool2d((1, 32))\n",
    "        self.pooling2 = nn.AdaptiveAvgPool2d((1, 1024))\n",
    "\n",
    "    def _load_model(self, model_name: str) -> nn.Module:\n",
    "        model_dict = {\n",
    "            \"efficientnet_b0\": models.efficientnet_b0,\n",
    "            \"efficientnet_b1\": models.efficientnet_b1,\n",
    "            \"efficientnet_b2\": models.efficientnet_b2,\n",
    "            \"efficientnet_b3\": models.efficientnet_b3,\n",
    "            \"efficientnet_b4\": models.efficientnet_b4,\n",
    "            \"efficientnet_b5\": models.efficientnet_b5,\n",
    "            \"efficientnet_b6\": models.efficientnet_b6,\n",
    "            \"efficientnet_b7\": models.efficientnet_b7,\n",
    "        }\n",
    "\n",
    "        if model_name not in model_dict:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported model_name '{model_name}'. Choose from: {list(model_dict.keys())}\"\n",
    "            )\n",
    "\n",
    "        model = model_dict[model_name](weights=\"DEFAULT\")\n",
    "        return model.features  # Only use the feature extractor part\n",
    "\n",
    "    def _build_transform(\n",
    "        self, target_size: int, central_fraction: float\n",
    "    ) -> nn.Sequential:\n",
    "        resize_size = int(target_size / central_fraction)\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(resize_size),\n",
    "                transforms.CenterCrop(target_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def freeze(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, images: Image.Image) -> torch.Tensor:\n",
    "        images_tensor = torch.stack(\n",
    "            [self.transform(image.convert(\"RGB\")) for image in images]\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = self.model(images_tensor)\n",
    "\n",
    "        features = self.pooling1(features)\n",
    "        features = features.permute(0, 3, 2, 1)\n",
    "        features = self.pooling2(features)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        flattened = features.view(batch_size, features.shape[1], -1)\n",
    "\n",
    "        return flattened\n",
    "\n",
    "\n",
    "class Blip2ViTExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Blip2ViTExtractor, self).__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model, self.preprocess, _ = load_model_and_preprocess(\n",
    "            name=\"blip2_feature_extractor\",\n",
    "            model_type=\"pretrain\",\n",
    "            is_eval=True,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.preprocess = self.preprocess[\"eval\"]\n",
    "\n",
    "        # üëá Th√™m l·ªõp Linear ƒë·ªÉ chuy·ªÉn t·ª´ 768 ‚Üí 1024\n",
    "        self.linear_proj = nn.Linear(768, 1024)\n",
    "\n",
    "    def freeze(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, images):\n",
    "        images = torch.stack(\n",
    "            [self.preprocess(image.convert(\"RGB\")).to(self.device) for image in images]\n",
    "        )\n",
    "\n",
    "        image_features = self.model.extract_features(\n",
    "            samples={\"image\": images}, mode=\"image\"\n",
    "        ).image_embeds  # shape: (B, N, 768)\n",
    "\n",
    "        image_features = self.linear_proj(image_features)  # shape: (B, N, 1024)\n",
    "        return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c50edde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.460118Z",
     "iopub.status.busy": "2025-05-13T04:18:59.459805Z",
     "iopub.status.idle": "2025-05-13T04:18:59.568758Z",
     "shell.execute_reply": "2025-05-13T04:18:59.567903Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.460088Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BARTPho(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"vinai/bartpho-syllable\",\n",
    "        device=\"cpu\",\n",
    "        max_length=50,\n",
    "        use_lora=False,\n",
    "        lora_r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Load base model\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "        # Apply LoRA if needed\n",
    "        if use_lora:\n",
    "            lora_config = LoraConfig(\n",
    "                r=lora_r,\n",
    "                lora_alpha=lora_alpha,\n",
    "                lora_dropout=lora_dropout,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                target_modules=target_modules,\n",
    "            )\n",
    "            base_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "        # Save full model\n",
    "        self.encoder = base_model.base_model.model.model.encoder.to(device)\n",
    "        self.decoder = base_model.base_model.model.model.decoder  # BART/M-BART decoder\n",
    "        self.lm_head = base_model.base_model.model.lm_head\n",
    "\n",
    "    def encode(self, input_texts):\n",
    "        batch_encoder_hidden_states = []\n",
    "        batch_attention_masks = []\n",
    "        batch_input_ids = []\n",
    "\n",
    "        for text in input_texts:\n",
    "            # Split sentence into chunks (each chunk has at most 512 words)\n",
    "            words = text.split()\n",
    "            chunks = [\n",
    "                \" \".join(words[i * 384 : (i + 1) * 384])\n",
    "                for i in range((len(words) + 383) // 384)\n",
    "            ]\n",
    "\n",
    "            chunk_input_ids_list = []\n",
    "            chunk_attention_mask_list = []\n",
    "            chunk_hidden_states_list = []\n",
    "\n",
    "            for chunk in chunks:\n",
    "                encoded = self.tokenizer(\n",
    "                    chunk, return_tensors=\"pt\", padding=True, truncation=True\n",
    "                ).to(self.device)\n",
    "\n",
    "                input_ids = encoded[\"input_ids\"]  # shape: (1, seq_len)\n",
    "                attention_mask = encoded[\"attention_mask\"]  # shape: (1, seq_len)\n",
    "\n",
    "                outputs = self.encoder(\n",
    "                    input_ids=input_ids, attention_mask=attention_mask\n",
    "                )\n",
    "\n",
    "                chunk_input_ids_list.append(input_ids)\n",
    "                chunk_attention_mask_list.append(attention_mask)\n",
    "                chunk_hidden_states_list.append(outputs.last_hidden_state)\n",
    "\n",
    "            # Concatenate all chunks ‚Üí shape: (1, total_seq_len, hidden_size)\n",
    "            full_input_ids = torch.cat(chunk_input_ids_list, dim=1).squeeze(\n",
    "                0\n",
    "            )  # shape: (total_seq_len)\n",
    "            full_attention_mask = torch.cat(chunk_attention_mask_list, dim=1).squeeze(\n",
    "                0\n",
    "            )  # shape: (total_seq_len)\n",
    "            full_hidden_states = torch.cat(chunk_hidden_states_list, dim=1).squeeze(\n",
    "                0\n",
    "            )  # shape: (total_seq_len, hidden_size)\n",
    "\n",
    "            batch_input_ids.append(full_input_ids)\n",
    "            batch_attention_masks.append(full_attention_mask)\n",
    "            batch_encoder_hidden_states.append(full_hidden_states)\n",
    "\n",
    "        # Pad sequences in the batch to the same length\n",
    "        max_seq_len = max(x.shape[0] for x in batch_input_ids)\n",
    "\n",
    "        def pad_tensor(tensor, max_len, pad_value=0):\n",
    "            pad_len = max_len - tensor.shape[0]\n",
    "            if pad_len == 0:\n",
    "                return tensor\n",
    "            pad = torch.full(\n",
    "                (pad_len,), pad_value, dtype=tensor.dtype, device=tensor.device\n",
    "            )\n",
    "            return torch.cat([tensor, pad], dim=0)\n",
    "\n",
    "        def pad_hidden_states(tensor, max_len):\n",
    "            pad_len = max_len - tensor.shape[0]\n",
    "            if pad_len == 0:\n",
    "                return tensor\n",
    "            pad = torch.zeros((pad_len, tensor.shape[1]), device=tensor.device)\n",
    "            return torch.cat([tensor, pad], dim=0)\n",
    "\n",
    "        # Stack padded tensors\n",
    "        input_ids = torch.stack(\n",
    "            [\n",
    "                pad_tensor(x, max_seq_len, self.tokenizer.pad_token_id)\n",
    "                for x in batch_input_ids\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        attention_mask = torch.stack(\n",
    "            [pad_tensor(x, max_seq_len, 0) for x in batch_attention_masks], dim=0\n",
    "        )\n",
    "        encoder_hidden_states = torch.stack(\n",
    "            [pad_hidden_states(x, max_seq_len) for x in batch_encoder_hidden_states],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"encoder_hidden_states\": encoder_hidden_states,  # shape: (batch_size, seq_len, hidden_size)\n",
    "            \"attention_mask\": attention_mask,  # shape: (batch_size, seq_len)\n",
    "            \"input_ids\": input_ids,  # shape: (batch_size, seq_len)\n",
    "        }\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        answer_input_ids,\n",
    "        answer_attention_mask,\n",
    "        encoder_hidden_states,\n",
    "        encoder_attention_mask,\n",
    "    ):\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=answer_input_ids,\n",
    "            attention_mask=answer_attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "        )\n",
    "\n",
    "        logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, encoder_hidden_states, encoder_attention_mask):\n",
    "        batch_size = encoder_hidden_states.size(0)\n",
    "        device = encoder_hidden_states.device\n",
    "\n",
    "        # B·∫Øt ƒë·∫ßu v·ªõi decoder_input_ids l√† eos_token_id cho m·ªói d√≤ng trong batch\n",
    "        decoder_input_ids = torch.full(\n",
    "            (batch_size, 1),\n",
    "            fill_value=self.tokenizer.eos_token_id,\n",
    "            dtype=torch.long,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(self.max_length):\n",
    "                logits = self.decode(\n",
    "                    answer_input_ids=decoder_input_ids,\n",
    "                    answer_attention_mask=torch.ones_like(\n",
    "                        decoder_input_ids, device=device\n",
    "                    ),\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask,\n",
    "                )\n",
    "\n",
    "                next_token = logits[:, -1, :].argmax(-1, keepdim=True)  # (B, 1)\n",
    "                decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=-1)\n",
    "\n",
    "                # D·ª´ng s·ªõm n·∫øu t·∫•t c·∫£ d√≤ng ƒë·ªÅu sinh ra <eos>\n",
    "                if (next_token == self.tokenizer.eos_token_id).all():\n",
    "                    break\n",
    "\n",
    "        return self.tokenizer.batch_decode(decoder_input_ids, skip_special_tokens=True)\n",
    "\n",
    "    def freeze_encoder(self, layers_to_freeze=None):\n",
    "        if layers_to_freeze is None:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for idx, layer in enumerate(self.encoder.layers):\n",
    "                if idx in layers_to_freeze:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def unfreeze_encoder(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db908ef3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.569861Z",
     "iopub.status.busy": "2025-05-13T04:18:59.569600Z",
     "iopub.status.idle": "2025-05-13T04:18:59.590716Z",
     "shell.execute_reply": "2025-05-13T04:18:59.589942Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.569840Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model=768, d_ff=2048, dropout=0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dropout1(F.gelu(self.fc1(x)))\n",
    "        out = self.dropout2(self.fc2(out))\n",
    "        return self.norm(x + out)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=768, num_heads=8, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask=None):\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = (\n",
    "                ~attention_mask.bool()\n",
    "            )  # Convert to padding mask: True = MASK\n",
    "        attn_output, _ = self.attn(\n",
    "            queries, keys, values, key_padding_mask=attention_mask\n",
    "        )\n",
    "        out = self.dropout(attn_output)\n",
    "        return self.norm(queries + out)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=768, num_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mhatt = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.pwff = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask=None):\n",
    "        att = self.mhatt(queries, keys, values, attention_mask)\n",
    "        ff = self.pwff(att)\n",
    "        return ff\n",
    "\n",
    "\n",
    "class BiDirectionalCrossAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=1024,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        dropout=0.1,\n",
    "        num_layers=3,\n",
    "        max_len=6000,\n",
    "    ):\n",
    "        super(BiDirectionalCrossAttention, self).__init__()\n",
    "\n",
    "        self.vision_pos_embed = nn.Embedding(max_len, d_model)\n",
    "        self.text_pos_embed = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        self.vision_norm = nn.LayerNorm(d_model)\n",
    "        self.text_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.d_model = d_model  # D = 1024\n",
    "        # self.text_proj = nn.Linear(1024, self.d_model)  # ‚ùå Lo·∫°i b·ªè v√¨ kh√¥ng c·∫ßn n·ªØa\n",
    "\n",
    "        self.vision_language_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.language_vision_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.vision_self_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.language_self_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, vision_feats, vision_mask, text_feats, text_mask):\n",
    "        batch_size, v_len, _ = vision_feats.size()\n",
    "        _, t_len, _ = text_feats.size()\n",
    "\n",
    "        v_pos_ids = (\n",
    "            torch.arange(v_len, device=vision_feats.device)\n",
    "            .unsqueeze(0)\n",
    "            .repeat(batch_size, 1)\n",
    "        )\n",
    "        t_pos_ids = (\n",
    "            torch.arange(t_len, device=text_feats.device)\n",
    "            .unsqueeze(0)\n",
    "            .repeat(batch_size, 1)\n",
    "        )\n",
    "\n",
    "        vision_feats = self.vision_norm(vision_feats + self.vision_pos_embed(v_pos_ids))\n",
    "        text_feats = self.text_norm(text_feats + self.text_pos_embed(t_pos_ids))\n",
    "\n",
    "        for vl_attn, lv_attn, v_self, l_self in zip(\n",
    "            self.vision_language_attn_layers,\n",
    "            self.language_vision_attn_layers,\n",
    "            self.vision_self_attn_layers,\n",
    "            self.language_self_attn_layers,\n",
    "        ):\n",
    "            vision_feats = vl_attn(vision_feats, text_feats, text_feats, text_mask)\n",
    "            text_feats = lv_attn(text_feats, vision_feats, vision_feats, vision_mask)\n",
    "\n",
    "            vision_feats = v_self(vision_feats, vision_feats, vision_feats, vision_mask)\n",
    "            text_feats = l_self(text_feats, text_feats, text_feats, text_mask)\n",
    "\n",
    "        fused_feats = torch.cat([vision_feats, text_feats], dim=1)  # shape: (B, V+T, D)\n",
    "\n",
    "        return fused_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb6e91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.591859Z",
     "iopub.status.busy": "2025-05-13T04:18:59.591605Z",
     "iopub.status.idle": "2025-05-13T04:18:59.618916Z",
     "shell.execute_reply": "2025-05-13T04:18:59.618119Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.591838Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NonTextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NonTextModel, self).__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.local_visual_extractor = EfficientNetFeatureExtractor(\n",
    "            model_name=\"efficientnet_b7\"\n",
    "        ).to(self.device)\n",
    "        self.global_visual_extractor = Blip2ViTExtractor().to(self.device)\n",
    "\n",
    "        self.bart_pho = BARTPho(device=self.device, use_lora=True)  # truy·ªÅn r√µ device\n",
    "        self.encoder = BiDirectionalCrossAttention(max_len=1028).to(self.device)\n",
    "\n",
    "    def forward(self, images, questions, answers):\n",
    "        local_features = self.local_visual_extractor(images)\n",
    "        global_features = self.global_visual_extractor(images)\n",
    "        vision_feats = torch.cat([local_features, global_features], dim=1)\n",
    "        vision_feats = vision_feats.to(self.device)\n",
    "        vision_mask = torch.ones(vision_feats.size()[:-1], dtype=torch.bool).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        text_encoding = self.bart_pho.encode(questions)\n",
    "        text_feats = text_encoding[\"encoder_hidden_states\"]\n",
    "        question_attention_mask = text_encoding[\"attention_mask\"]\n",
    "\n",
    "        encoder_output = self.encoder(\n",
    "            vision_feats, vision_mask, text_feats, question_attention_mask\n",
    "        )\n",
    "        encoder_attention_mask = torch.cat(\n",
    "            [vision_mask, question_attention_mask], dim=1\n",
    "        )\n",
    "\n",
    "        answer_encoded = self.bart_pho.tokenizer(\n",
    "            answers, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).to(self.device)\n",
    "        answer_input_ids = answer_encoded[\"input_ids\"]\n",
    "        answer_attention_mask = answer_encoded[\"attention_mask\"]\n",
    "\n",
    "        logits = self.bart_pho.decode(\n",
    "            answer_input_ids=answer_input_ids,\n",
    "            answer_attention_mask=answer_attention_mask,\n",
    "            encoder_hidden_states=encoder_output,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "        )\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(self, images, questions):\n",
    "        # Step 1: Extract visual features\n",
    "        with torch.no_grad():\n",
    "            local_features = self.local_visual_extractor(images)  # (B, N, D)\n",
    "            global_features = self.global_visual_extractor(images)  # (B, 1, D)\n",
    "            vision_feats = torch.cat([local_features, global_features], dim=1)\n",
    "            vision_mask = torch.ones(vision_feats.size()[:-1], dtype=torch.bool).to(\n",
    "                self.device\n",
    "            )\n",
    "\n",
    "            text_encoding = self.bart_pho.encode(questions)\n",
    "            text_feats = text_encoding[\"encoder_hidden_states\"]\n",
    "            question_attention_mask = text_encoding[\"attention_mask\"]\n",
    "\n",
    "            # Step 3: Co-Attention Fusion\n",
    "            encoder_output = self.encoder(\n",
    "                vision_feats, vision_mask, text_feats, question_attention_mask\n",
    "            ).to(self.device)\n",
    "            encoder_attention_mask = torch.cat(\n",
    "                [vision_mask, question_attention_mask], dim=1\n",
    "            ).to(self.device)\n",
    "\n",
    "            return self.bart_pho.generate(encoder_output, encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714b6de-8e1c-4e85-918d-d4f82cb829f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.620007Z",
     "iopub.status.busy": "2025-05-13T04:18:59.619776Z",
     "iopub.status.idle": "2025-05-13T04:18:59.637128Z",
     "shell.execute_reply": "2025-05-13T04:18:59.636339Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.619988Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextModel, self).__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.bart_pho = BARTPho(device=self.device, use_lora=True)  # truy·ªÅn r√µ device\n",
    "        self.bart_pho.freeze_encoder()\n",
    "        self.encoder = BiDirectionalCrossAttention().to(self.device)\n",
    "\n",
    "    def forward(self, ocr_infos, questions, answers):\n",
    "        ocr_encoding = self.bart_pho.encode(ocr_infos)\n",
    "        ocr_feats = ocr_encoding[\"encoder_hidden_states\"]\n",
    "        ocr_attention_mask = ocr_encoding[\"attention_mask\"]\n",
    "\n",
    "        question_encoding = self.bart_pho.encode(questions)\n",
    "        question_feats = question_encoding[\"encoder_hidden_states\"]\n",
    "        question_attention_mask = question_encoding[\"attention_mask\"]\n",
    "\n",
    "        encoder_output = self.encoder(\n",
    "            ocr_feats, ocr_attention_mask, question_feats, question_attention_mask\n",
    "        )\n",
    "        encoder_attention_mask = torch.cat(\n",
    "            [ocr_attention_mask, question_attention_mask], dim=1\n",
    "        )\n",
    "\n",
    "        answer_encoded = self.bart_pho.tokenizer(\n",
    "            answers, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).to(self.device)\n",
    "        answer_input_ids = answer_encoded[\"input_ids\"]\n",
    "        answer_attention_mask = answer_encoded[\"attention_mask\"]\n",
    "\n",
    "        logits = self.bart_pho.decode(\n",
    "            answer_input_ids=answer_input_ids,\n",
    "            answer_attention_mask=answer_attention_mask,\n",
    "            encoder_hidden_states=encoder_output,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "        )\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(self, ocr_infos, questions):\n",
    "        # Step 1: Extract visual features\n",
    "        with torch.no_grad():\n",
    "            ocr_encoding = self.bart_pho.encode(ocr_infos)\n",
    "            ocr_feats = ocr_encoding[\"encoder_hidden_states\"]\n",
    "            ocr_attention_mask = ocr_encoding[\"attention_mask\"]\n",
    "\n",
    "            question_encoding = self.bart_pho.encode(questions)\n",
    "            question_feats = question_encoding[\"encoder_hidden_states\"]\n",
    "            question_attention_mask = question_encoding[\"attention_mask\"]\n",
    "\n",
    "            encoder_output = self.encoder(\n",
    "                ocr_feats, ocr_attention_mask, question_feats, question_attention_mask\n",
    "            )\n",
    "            encoder_attention_mask = torch.cat(\n",
    "                [ocr_attention_mask, question_attention_mask], dim=1\n",
    "            )\n",
    "\n",
    "            return self.bart_pho.generate(encoder_output, encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181904e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.638127Z",
     "iopub.status.busy": "2025-05-13T04:18:59.637902Z",
     "iopub.status.idle": "2025-05-13T04:18:59.658753Z",
     "shell.execute_reply": "2025-05-13T04:18:59.657998Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.638104Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_type_prompt(question: str):\n",
    "    instructions = \"\"\"\n",
    "    B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c m·ªôt c√¢u h·ªèi. H√£y ph√¢n lo·∫°i xem c√¢u h·ªèi ƒë√≥ l√† \"text\" ho·∫∑c l√† \"non-text\".\n",
    "    \n",
    "    **ƒê·ªãnh nghƒ©a**:\n",
    "    - \"text\": bao g·ªìm c√°c c√¢u h·ªèi d·ª±a tr√™n c√°c y·∫øu t·ªë tr√≠ch xu·∫•t t·ª´ vƒÉn b·∫£n, li√™n quan ƒë·∫øn c√°c s·ªë li·ªáu, t·ª´ ng·ªØ, ho·∫∑c b·∫•t k·ª≥ h√¨nh th·ª©c vƒÉn b·∫£n n√†o.\n",
    "    - \"non-text\": bao g·ªìm c√°c c√¢u h·ªèi kh√¥ng d·ª±a tr√™n c√°c y·∫øu t·ªë vƒÉn b·∫£n, ch·∫≥ng h·∫°n nh∆∞ c√°c c√¢u h·ªèi d·ª±a tr√™n h√¨nh ·∫£nh, v·∫≠t th·ªÉ, m√†u s·∫Øc, h√¨nh d·∫°ng, v.v.\n",
    "    \n",
    "    Tr·∫£ l·ªùi ƒë·∫ßu ra l√† m·ªôt t·ª´ duy nh·∫•t: \"text\" ho·∫∑c \"non-text\".\n",
    "    \n",
    "    D∆∞·ªõi ƒë√¢y l√† c√¢u h·ªèi c·∫ßn ph√¢n lo·∫°i:\n",
    "    \"\"\".strip()\n",
    "\n",
    "    ending = '\\n\\nH√£y tr·∫£ v·ªÅ 1 t·ª´ duy nh·∫•t: \"text\" ho·∫∑c \"non-text\".'\n",
    "\n",
    "    full_prompt = f\"{instructions}\\n{question}{ending}\"\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ff3af1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.660016Z",
     "iopub.status.busy": "2025-05-13T04:18:59.659506Z",
     "iopub.status.idle": "2025-05-13T04:18:59.684303Z",
     "shell.execute_reply": "2025-05-13T04:18:59.683532Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.660000Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_ocr_prompt(ocr_chunks: list[str]) -> str:\n",
    "    prompt = f\"\"\"B·∫°n nh·∫≠n ƒë∆∞·ª£c m·ªôt danh s√°ch vƒÉn b·∫£n ng·∫Øn, tr√≠ch xu·∫•t t·ª´ ·∫£nh infographic (OCR).\n",
    "            \n",
    "    Y√™u c·∫ßu:\n",
    "    - Gom nh√≥m c√°c d√≤ng c√≥ li√™n quan theo ng·ªØ c·∫£nh.\n",
    "    - Vi·∫øt l·∫°i th√†nh c√°c c√¢u ho√†n ch·ªânh, ng·∫Øn g·ªçn, r√µ nghƒ©a d√πng ƒë·ªÉ th·ª±c hi·ªán text embedding.\n",
    "    - Tr·∫£ v·ªÅ **duy nh·∫•t m·ªôt Python list h·ª£p l·ªá** ch·ª©a c√°c c√¢u, v√≠ d·ª•:\n",
    "    ```python\n",
    "    [\n",
    "        \"C√¢u ho√†n ch·ªânh 1.\",\n",
    "        \"C√¢u ho√†n ch·ªânh 2.\",\n",
    "        ...\n",
    "    ]\n",
    "    \n",
    "    D·ªØ li·ªáu:\n",
    "    {ocr_chunks}\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30743312",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.685155Z",
     "iopub.status.busy": "2025-05-13T04:18:59.684971Z",
     "iopub.status.idle": "2025-05-13T04:18:59.700498Z",
     "shell.execute_reply": "2025-05-13T04:18:59.699622Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.685141Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fix_list_string(raw_str: str) -> str:\n",
    "    # B∆∞·ªõc 1: D·ªçn s·∫°ch chu·ªói ƒë·∫ßu v√†o\n",
    "    raw_str = raw_str.strip()\n",
    "\n",
    "    # B∆∞·ªõc 2: Ch√®n d·∫•u ph·∫©y gi·ªØa c√°c d·∫•u \" li·ªÅn nhau n·∫øu c√≥\n",
    "    raw_str = re.sub(r'\"\\s*\"', '\", \"', raw_str)\n",
    "\n",
    "    # B∆∞·ªõc 3: Parse t·ª´ng d√≤ng\n",
    "    lines = raw_str.strip(\"[]\").split(\"\\n\")\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip().rstrip(\",\")\n",
    "\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Lo·∫°i b·ªè d·∫•u \" bao ngo√†i n·∫øu c√≥\n",
    "        if line.startswith('\"') and line.endswith('\"'):\n",
    "            line = line[1:-1]\n",
    "\n",
    "        # Escape c√°c d·∫•u \" b√™n trong n·ªôi dung\n",
    "        line = line.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n",
    "\n",
    "        # Th√™m l·∫°i d·∫•u \" bao ngo√†i\n",
    "        line = f'\"{line}\"'\n",
    "\n",
    "        # Th√™m d·∫•u ph·∫©y n·∫øu ch∆∞a ph·∫£i d√≤ng cu·ªëi\n",
    "        if i < len(lines) - 1:\n",
    "            line += \",\"\n",
    "\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    fixed_str = \"[\\n\" + \"\\n\".join(cleaned_lines) + \"\\n]\"\n",
    "    return fixed_str\n",
    "\n",
    "\n",
    "def clean_ocr_contents(response: str) -> str:\n",
    "    response = re.sub(r\"^```python\\s*\", \"\", response.strip(), flags=re.IGNORECASE)\n",
    "    response = re.sub(r\"\\s*```$\", \"\", response.strip())\n",
    "    # Thay d·∫•u ngo·∫∑c k√©p cong th√†nh th·∫≥ng\n",
    "    response = response.replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"')\n",
    "    # Th·ª±c thi parsing\n",
    "    response = fix_list_string(response)\n",
    "    meaning_sentences = ast.literal_eval(response)\n",
    "    joined_text = \" \".join(meaning_sentences)\n",
    "\n",
    "    return joined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed292c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.701684Z",
     "iopub.status.busy": "2025-05-13T04:18:59.701336Z",
     "iopub.status.idle": "2025-05-13T04:20:55.224834Z",
     "shell.execute_reply": "2025-05-13T04:20:55.224053Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.701659Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b7_lukemelas-c5b4e57e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b7_lukemelas-c5b4e57e.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 255M/255M [00:02<00:00, 93.6MB/s] \n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04426ac70be642acb0718735743346f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e174aeabc294ced9dc70283c439b092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a5f6ca82a74820a1ba5a005010e6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.89G/1.89G [00:47<00:00, 42.7MB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/lavis/models/eva_vit.py:433: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cached_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa5b30137c845cab52e3ee2c09f623d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 712M/712M [00:17<00:00, 43.2MB/s] \n",
      "/usr/local/lib/python3.11/dist-packages/lavis/models/blip2_models/blip2.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(cached_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce371cc8acaa483a97f0cb82422ac245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/897 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17eb00fef21b4db3b39dae579f9345c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb114946e71495d90fe5beaca3cf228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dict.txt:   0%|          | 0.00/360k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a276011f40b403ab69b949c7045204e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.58G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "/tmp/ipykernel_31/4157559533.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath, map_location=DEVICE)\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, filepath):\n",
    "    #\n",
    "    if os.path.isfile(filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=DEVICE)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "\n",
    "#\n",
    "TEXT_MODEL = TextModel().to(DEVICE)\n",
    "load_checkpoint(TEXT_MODEL, TEXT_CHECKPOINT_PATH)\n",
    "TEXT_MODEL.eval()\n",
    "\n",
    "#\n",
    "NONTEXT_MODEL = NonTextModel().to(DEVICE)\n",
    "load_checkpoint(NONTEXT_MODEL, NONTEXT_CHECKPOINT_PATH)\n",
    "NONTEXT_MODEL.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af900c19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:20:55.230435Z",
     "iopub.status.busy": "2025-05-13T04:20:55.230192Z",
     "iopub.status.idle": "2025-05-13T04:20:55.250158Z",
     "shell.execute_reply": "2025-05-13T04:20:55.249529Z",
     "shell.execute_reply.started": "2025-05-13T04:20:55.230414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_answer(image: Image, question: str) -> str:\n",
    "    #\n",
    "    prompt = build_type_prompt(question)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    question_type = CLIENT.models.generate_content(\n",
    "        model=GEMINI_MODEL, contents=prompt, config=GENERATION_CONFIG\n",
    "    ).text.strip()\n",
    "    print(f\"Question types: {question_type}\")\n",
    "\n",
    "    #\n",
    "    if question_type.lower() == \"text\":\n",
    "        #\n",
    "        image_np = np.array(image)\n",
    "        ocr_chunks = OCR_READER.readtext(image_np)\n",
    "        ocr_chunks = list(chunk[1] for chunk in ocr_chunks)\n",
    "\n",
    "        #\n",
    "        ocr_prompt = build_ocr_prompt(ocr_chunks)\n",
    "        contents = [ocr_prompt, image]\n",
    "\n",
    "        try:\n",
    "            #\n",
    "            response = CLIENT.models.generate_content(\n",
    "                model=GEMINI_MODEL, contents=contents, config=GENERATION_CONFIG\n",
    "            )\n",
    "            response = response.text\n",
    "            ocr_info = clean_ocr_contents(response)\n",
    "            print(f\"OCR \")\n",
    "        except:\n",
    "            return (\n",
    "                \"Your image may contain sensitive or violent content, please change it!\"\n",
    "            )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            answer = TEXT_MODEL.generate([ocr_info], [question])\n",
    "            print(f\"Text model responded\")\n",
    "        return answer\n",
    "    elif question_type.lower() == \"non-text\":\n",
    "        with torch.no_grad():\n",
    "            answer = NONTEXT_MODEL.generate([image], [question])\n",
    "            print(f\"Non-text model responded\")\n",
    "        return answer\n",
    "    else:\n",
    "        return \"Please try again!\"\n",
    "\n",
    "\n",
    "# def generate_answer(image: Image, question: str) -> str:\n",
    "#     with torch.no_grad():\n",
    "#         answer = NONTEXT_MODEL.generate([image], [question])\n",
    "#         print(f\"Non-text model responded\")\n",
    "#     return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d375a-c9e0-4c2a-b37b-b43f60d78ec1",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf39b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q fastapi uvicorn nest-asyncio pyngrok pillow\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d3fe6-7da2-4b15-a041-30b5fd10d2d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:20:59.226090Z",
     "iopub.status.busy": "2025-05-13T04:20:59.225883Z",
     "iopub.status.idle": "2025-05-13T04:21:00.918860Z",
     "shell.execute_reply": "2025-05-13T04:21:00.917909Z",
     "shell.execute_reply.started": "2025-05-13T04:20:59.226069Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n"
     ]
    }
   ],
   "source": [
    "!ngrok config add-authtoken <YOUR_NGROK_TOKEN_KEY_HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16747461-2fec-4baa-80a3-4692f2337c09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:21:00.920832Z",
     "iopub.status.busy": "2025-05-13T04:21:00.920075Z",
     "iopub.status.idle": "2025-05-13T04:21:01.169157Z",
     "shell.execute_reply": "2025-05-13T04:21:01.168390Z",
     "shell.execute_reply.started": "2025-05-13T04:21:00.920804Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import gc\n",
    "import base64\n",
    "from PIL import Image\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "# Allow nested async event loop (required for Jupyter/Kaggle)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "class InferenceRequest(BaseModel):\n",
    "    questions: list[str]\n",
    "    image_base64: str\n",
    "\n",
    "\n",
    "class InferenceResponse(BaseModel):\n",
    "    answers: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd843e1-61be-46f2-a15d-4a28cf1c5781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:21:01.172367Z",
     "iopub.status.busy": "2025-05-13T04:21:01.172146Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [31]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NgrokTunnel: \"https://204d-34-141-197-202.ngrok-free.app\" -> \"http://localhost:8000\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n"
     ]
    }
   ],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "# Enable CORS for development (allow all)\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "def decode_image(base64_string: str) -> Image:\n",
    "    try:\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        image = Image.open(io.BytesIO(image_data)).convert(\"RGB\")\n",
    "        print(f\"Image decoded\")\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"Invalid base64 image input\")\n",
    "\n",
    "\n",
    "@app.post(\"/infer\", response_model=InferenceResponse)\n",
    "def infer(request: InferenceRequest):\n",
    "    try:\n",
    "        # print(request.question)\n",
    "        image = decode_image(request.image_base64)\n",
    "\n",
    "        result = generate_answer(image, request.questions[0])\n",
    "        print(result)\n",
    "        del image\n",
    "        return InferenceResponse(answers=result)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "    finally:\n",
    "        # Explicit memory cleanup\n",
    "        gc.collect()  # Run Python garbage collection\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()  # Free up CUDA memory\n",
    "\n",
    "\n",
    "try:\n",
    "    ngrok.kill()\n",
    "    public_url = ngrok.connect(8000)\n",
    "    print(f\"{public_url}\")\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "except (KeyboardInterrupt, Exception):\n",
    "    ..."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7383612,
     "sourceId": 11761392,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
