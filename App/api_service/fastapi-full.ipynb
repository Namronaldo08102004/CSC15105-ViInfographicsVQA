{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4151008e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:15:48.462260Z",
     "iopub.status.busy": "2025-05-13T04:15:48.461992Z",
     "iopub.status.idle": "2025-05-13T04:18:10.989986Z",
     "shell.execute_reply": "2025-05-13T04:18:10.989078Z",
     "shell.execute_reply.started": "2025-05-13T04:15:48.462227Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install salesforce-lavis\n",
    "!pip install torch \n",
    "!pip install torchvision\n",
    "!pip install transformers\n",
    "!pip install peft==0.10.0\n",
    "!pip install datasets\n",
    "!pip install pillow\n",
    "!pip install underthesea\n",
    "!pip install huggingface_hub\n",
    "!pip install hf_xet\n",
    "!pip install google-genai\n",
    "!pip install easyocr\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c90aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:10.991297Z",
     "iopub.status.busy": "2025-05-13T04:18:10.991028Z",
     "iopub.status.idle": "2025-05-13T04:18:37.930840Z",
     "shell.execute_reply": "2025-05-13T04:18:37.930284Z",
     "shell.execute_reply.started": "2025-05-13T04:18:10.991269Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 04:18:19.240760: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747109899.467328      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747109899.534744      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/usr/local/lib/python3.11/dist-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
      "/usr/local/lib/python3.11/dist-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from lavis.models import load_model_and_preprocess\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import hf_hub_download, HfApi\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import GenerateContentConfig\n",
    "import easyocr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6242e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:37.932018Z",
     "iopub.status.busy": "2025-05-13T04:18:37.931554Z",
     "iopub.status.idle": "2025-05-13T04:18:37.936164Z",
     "shell.execute_reply": "2025-05-13T04:18:37.935241Z",
     "shell.execute_reply.started": "2025-05-13T04:18:37.931994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define project-specific variables\n",
    "PROJECT_NAME = \"ViInfographicsVQA\"  # Name of the project\n",
    "USERNAME = \"Namronaldo2004\"  # Hugging Face username\n",
    "HUGGINGFACE_HUB_REPO = (\n",
    "    USERNAME + \"/\" + PROJECT_NAME\n",
    ")  # Full repository name on Hugging Face Hub\n",
    "TEXT_BASELINE_NAME = \"Flow3-modified/Text\"\n",
    "TEXT_CHECKPOINT_FILENAME = f\"{TEXT_BASELINE_NAME}/latest_checkpoint.pth\"\n",
    "NONTEXT_BASELINE_NAME = \"Flow3-modified/Non-text\"\n",
    "NONTEXT_CHECKPOINT_FILENAME = f\"{NONTEXT_BASELINE_NAME}/latest_checkpoint.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8021d4eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:37.937653Z",
     "iopub.status.busy": "2025-05-13T04:18:37.937052Z",
     "iopub.status.idle": "2025-05-13T04:18:38.139676Z",
     "shell.execute_reply": "2025-05-13T04:18:38.138772Z",
     "shell.execute_reply.started": "2025-05-13T04:18:37.937625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "OCR_READER = easyocr.Reader([\"vi\"])\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "CLIENT = genai.Client(api_key=GEMINI_API_KEY)\n",
    "GEMINI_MODEL = \"gemini-2.5-flash-preview-04-17\"\n",
    "GENERATION_CONFIG = GenerateContentConfig(response_mime_type=\"text/plain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057495d4-cbdb-424d-98d5-f99c92ab326f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:38.140812Z",
     "iopub.status.busy": "2025-05-13T04:18:38.140453Z",
     "iopub.status.idle": "2025-05-13T04:18:59.443717Z",
     "shell.execute_reply": "2025-05-13T04:18:59.442874Z",
     "shell.execute_reply.started": "2025-05-13T04:18:38.140792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/Flow3-modified/Non-text/latest_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "api = HfApi()\n",
    "NONTEXT_CHECKPOINT_PATH = \"./checkpoints/nontext_latest_checkpoint.pth\"\n",
    "TEXT_CHECKPOINT_PATH = \"./checkpoints/text_latest_checkpoint.pth\"\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "if not os.path.exists(NONTEXT_CHECKPOINT_PATH):\n",
    "    if api.file_exists(\n",
    "        repo_id=HUGGINGFACE_HUB_REPO,\n",
    "        filename=NONTEXT_CHECKPOINT_FILENAME,\n",
    "        repo_type=\"model\",\n",
    "    ):\n",
    "        NONTEXT_CHECKPOINT_PATH = hf_hub_download(\n",
    "            repo_id=HUGGINGFACE_HUB_REPO,\n",
    "            filename=NONTEXT_CHECKPOINT_FILENAME,\n",
    "            local_dir=\"./checkpoints\",  # Store the checkpoint locally in the \"checkpoints\" directory\n",
    "        )\n",
    "print(NONTEXT_CHECKPOINT_PATH)\n",
    "# =======================================================================================================\n",
    "if not os.path.exists(TEXT_CHECKPOINT_PATH):\n",
    "    if api.file_exists(\n",
    "        repo_id=HUGGINGFACE_HUB_REPO,\n",
    "        filename=TEXT_CHECKPOINT_FILENAME,\n",
    "        repo_type=\"model\",\n",
    "    ):\n",
    "        TEXT_CHECKPOINT_PATH = hf_hub_download(\n",
    "            repo_id=HUGGINGFACE_HUB_REPO,\n",
    "            filename=TEXT_CHECKPOINT_FILENAME,\n",
    "            local_dir=\"./checkpoints\",  # Store the checkpoint locally in the \"checkpoints\" directory\n",
    "        )\n",
    "print(TEXT_CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d23e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.446459Z",
     "iopub.status.busy": "2025-05-13T04:18:59.446233Z",
     "iopub.status.idle": "2025-05-13T04:18:59.459022Z",
     "shell.execute_reply": "2025-05-13T04:18:59.458141Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.446438Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EfficientNetFeatureExtractor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"efficientnet_b0\",\n",
    "        target_size: int = 224,\n",
    "        central_fraction: float = 0.875,\n",
    "    ):\n",
    "        super(EfficientNetFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self._load_model(model_name).to(self.device)\n",
    "        self.transform = self._build_transform(target_size, central_fraction)\n",
    "\n",
    "        self.pooling1 = nn.AdaptiveAvgPool2d((1, 32))\n",
    "        self.pooling2 = nn.AdaptiveAvgPool2d((1, 1024))\n",
    "\n",
    "    def _load_model(self, model_name: str) -> nn.Module:\n",
    "        model_dict = {\n",
    "            \"efficientnet_b0\": models.efficientnet_b0,\n",
    "            \"efficientnet_b1\": models.efficientnet_b1,\n",
    "            \"efficientnet_b2\": models.efficientnet_b2,\n",
    "            \"efficientnet_b3\": models.efficientnet_b3,\n",
    "            \"efficientnet_b4\": models.efficientnet_b4,\n",
    "            \"efficientnet_b5\": models.efficientnet_b5,\n",
    "            \"efficientnet_b6\": models.efficientnet_b6,\n",
    "            \"efficientnet_b7\": models.efficientnet_b7,\n",
    "        }\n",
    "\n",
    "        if model_name not in model_dict:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported model_name '{model_name}'. Choose from: {list(model_dict.keys())}\"\n",
    "            )\n",
    "\n",
    "        model = model_dict[model_name](weights=\"DEFAULT\")\n",
    "        return model.features  # Only use the feature extractor part\n",
    "\n",
    "    def _build_transform(\n",
    "        self, target_size: int, central_fraction: float\n",
    "    ) -> nn.Sequential:\n",
    "        resize_size = int(target_size / central_fraction)\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(resize_size),\n",
    "                transforms.CenterCrop(target_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def freeze(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, images: Image.Image) -> torch.Tensor:\n",
    "        images_tensor = torch.stack(\n",
    "            [self.transform(image.convert(\"RGB\")) for image in images]\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = self.model(images_tensor)\n",
    "\n",
    "        features = self.pooling1(features)\n",
    "        features = features.permute(0, 3, 2, 1)\n",
    "        features = self.pooling2(features)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        flattened = features.view(batch_size, features.shape[1], -1)\n",
    "\n",
    "        return flattened\n",
    "\n",
    "\n",
    "class Blip2ViTExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Blip2ViTExtractor, self).__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model, self.preprocess, _ = load_model_and_preprocess(\n",
    "            name=\"blip2_feature_extractor\",\n",
    "            model_type=\"pretrain\",\n",
    "            is_eval=True,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.preprocess = self.preprocess[\"eval\"]\n",
    "\n",
    "        # 👇 Thêm lớp Linear để chuyển từ 768 → 1024\n",
    "        self.linear_proj = nn.Linear(768, 1024)\n",
    "\n",
    "    def freeze(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, images):\n",
    "        images = torch.stack(\n",
    "            [self.preprocess(image.convert(\"RGB\")).to(self.device) for image in images]\n",
    "        )\n",
    "\n",
    "        image_features = self.model.extract_features(\n",
    "            samples={\"image\": images}, mode=\"image\"\n",
    "        ).image_embeds  # shape: (B, N, 768)\n",
    "\n",
    "        image_features = self.linear_proj(image_features)  # shape: (B, N, 1024)\n",
    "        return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c50edde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.460118Z",
     "iopub.status.busy": "2025-05-13T04:18:59.459805Z",
     "iopub.status.idle": "2025-05-13T04:18:59.568758Z",
     "shell.execute_reply": "2025-05-13T04:18:59.567903Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.460088Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BARTPho(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"vinai/bartpho-syllable\",\n",
    "        device=\"cpu\",\n",
    "        max_length=50,\n",
    "        use_lora=False,\n",
    "        lora_r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Load base model\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "        # Apply LoRA if needed\n",
    "        if use_lora:\n",
    "            lora_config = LoraConfig(\n",
    "                r=lora_r,\n",
    "                lora_alpha=lora_alpha,\n",
    "                lora_dropout=lora_dropout,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                target_modules=target_modules,\n",
    "            )\n",
    "            base_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "        # Save full model\n",
    "        self.encoder = base_model.base_model.model.model.encoder.to(device)\n",
    "        self.decoder = base_model.base_model.model.model.decoder  # BART/M-BART decoder\n",
    "        self.lm_head = base_model.base_model.model.lm_head\n",
    "\n",
    "    def encode(self, input_texts):\n",
    "        batch_encoder_hidden_states = []\n",
    "        batch_attention_masks = []\n",
    "        batch_input_ids = []\n",
    "\n",
    "        for text in input_texts:\n",
    "            # Split sentence into chunks (each chunk has at most 512 words)\n",
    "            words = text.split()\n",
    "            chunks = [\n",
    "                \" \".join(words[i * 384 : (i + 1) * 384])\n",
    "                for i in range((len(words) + 383) // 384)\n",
    "            ]\n",
    "\n",
    "            chunk_input_ids_list = []\n",
    "            chunk_attention_mask_list = []\n",
    "            chunk_hidden_states_list = []\n",
    "\n",
    "            for chunk in chunks:\n",
    "                encoded = self.tokenizer(\n",
    "                    chunk, return_tensors=\"pt\", padding=True, truncation=True\n",
    "                ).to(self.device)\n",
    "\n",
    "                input_ids = encoded[\"input_ids\"]  # shape: (1, seq_len)\n",
    "                attention_mask = encoded[\"attention_mask\"]  # shape: (1, seq_len)\n",
    "\n",
    "                outputs = self.encoder(\n",
    "                    input_ids=input_ids, attention_mask=attention_mask\n",
    "                )\n",
    "\n",
    "                chunk_input_ids_list.append(input_ids)\n",
    "                chunk_attention_mask_list.append(attention_mask)\n",
    "                chunk_hidden_states_list.append(outputs.last_hidden_state)\n",
    "\n",
    "            # Concatenate all chunks → shape: (1, total_seq_len, hidden_size)\n",
    "            full_input_ids = torch.cat(chunk_input_ids_list, dim=1).squeeze(\n",
    "                0\n",
    "            )  # shape: (total_seq_len)\n",
    "            full_attention_mask = torch.cat(chunk_attention_mask_list, dim=1).squeeze(\n",
    "                0\n",
    "            )  # shape: (total_seq_len)\n",
    "            full_hidden_states = torch.cat(chunk_hidden_states_list, dim=1).squeeze(\n",
    "                0\n",
    "            )  # shape: (total_seq_len, hidden_size)\n",
    "\n",
    "            batch_input_ids.append(full_input_ids)\n",
    "            batch_attention_masks.append(full_attention_mask)\n",
    "            batch_encoder_hidden_states.append(full_hidden_states)\n",
    "\n",
    "        # Pad sequences in the batch to the same length\n",
    "        max_seq_len = max(x.shape[0] for x in batch_input_ids)\n",
    "\n",
    "        def pad_tensor(tensor, max_len, pad_value=0):\n",
    "            pad_len = max_len - tensor.shape[0]\n",
    "            if pad_len == 0:\n",
    "                return tensor\n",
    "            pad = torch.full(\n",
    "                (pad_len,), pad_value, dtype=tensor.dtype, device=tensor.device\n",
    "            )\n",
    "            return torch.cat([tensor, pad], dim=0)\n",
    "\n",
    "        def pad_hidden_states(tensor, max_len):\n",
    "            pad_len = max_len - tensor.shape[0]\n",
    "            if pad_len == 0:\n",
    "                return tensor\n",
    "            pad = torch.zeros((pad_len, tensor.shape[1]), device=tensor.device)\n",
    "            return torch.cat([tensor, pad], dim=0)\n",
    "\n",
    "        # Stack padded tensors\n",
    "        input_ids = torch.stack(\n",
    "            [\n",
    "                pad_tensor(x, max_seq_len, self.tokenizer.pad_token_id)\n",
    "                for x in batch_input_ids\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        attention_mask = torch.stack(\n",
    "            [pad_tensor(x, max_seq_len, 0) for x in batch_attention_masks], dim=0\n",
    "        )\n",
    "        encoder_hidden_states = torch.stack(\n",
    "            [pad_hidden_states(x, max_seq_len) for x in batch_encoder_hidden_states],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"encoder_hidden_states\": encoder_hidden_states,  # shape: (batch_size, seq_len, hidden_size)\n",
    "            \"attention_mask\": attention_mask,  # shape: (batch_size, seq_len)\n",
    "            \"input_ids\": input_ids,  # shape: (batch_size, seq_len)\n",
    "        }\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        answer_input_ids,\n",
    "        answer_attention_mask,\n",
    "        encoder_hidden_states,\n",
    "        encoder_attention_mask,\n",
    "    ):\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=answer_input_ids,\n",
    "            attention_mask=answer_attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "        )\n",
    "\n",
    "        logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, encoder_hidden_states, encoder_attention_mask):\n",
    "        batch_size = encoder_hidden_states.size(0)\n",
    "        device = encoder_hidden_states.device\n",
    "\n",
    "        # Bắt đầu với decoder_input_ids là eos_token_id cho mỗi dòng trong batch\n",
    "        decoder_input_ids = torch.full(\n",
    "            (batch_size, 1),\n",
    "            fill_value=self.tokenizer.eos_token_id,\n",
    "            dtype=torch.long,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(self.max_length):\n",
    "                logits = self.decode(\n",
    "                    answer_input_ids=decoder_input_ids,\n",
    "                    answer_attention_mask=torch.ones_like(\n",
    "                        decoder_input_ids, device=device\n",
    "                    ),\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask,\n",
    "                )\n",
    "\n",
    "                next_token = logits[:, -1, :].argmax(-1, keepdim=True)  # (B, 1)\n",
    "                decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=-1)\n",
    "\n",
    "                # Dừng sớm nếu tất cả dòng đều sinh ra <eos>\n",
    "                if (next_token == self.tokenizer.eos_token_id).all():\n",
    "                    break\n",
    "\n",
    "        return self.tokenizer.batch_decode(decoder_input_ids, skip_special_tokens=True)\n",
    "\n",
    "    def freeze_encoder(self, layers_to_freeze=None):\n",
    "        if layers_to_freeze is None:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for idx, layer in enumerate(self.encoder.layers):\n",
    "                if idx in layers_to_freeze:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def unfreeze_encoder(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db908ef3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.569861Z",
     "iopub.status.busy": "2025-05-13T04:18:59.569600Z",
     "iopub.status.idle": "2025-05-13T04:18:59.590716Z",
     "shell.execute_reply": "2025-05-13T04:18:59.589942Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.569840Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model=768, d_ff=2048, dropout=0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dropout1(F.gelu(self.fc1(x)))\n",
    "        out = self.dropout2(self.fc2(out))\n",
    "        return self.norm(x + out)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=768, num_heads=8, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask=None):\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = (\n",
    "                ~attention_mask.bool()\n",
    "            )  # Convert to padding mask: True = MASK\n",
    "        attn_output, _ = self.attn(\n",
    "            queries, keys, values, key_padding_mask=attention_mask\n",
    "        )\n",
    "        out = self.dropout(attn_output)\n",
    "        return self.norm(queries + out)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=768, num_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mhatt = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.pwff = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask=None):\n",
    "        att = self.mhatt(queries, keys, values, attention_mask)\n",
    "        ff = self.pwff(att)\n",
    "        return ff\n",
    "\n",
    "\n",
    "class BiDirectionalCrossAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=1024,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        dropout=0.1,\n",
    "        num_layers=3,\n",
    "        max_len=6000,\n",
    "    ):\n",
    "        super(BiDirectionalCrossAttention, self).__init__()\n",
    "\n",
    "        self.vision_pos_embed = nn.Embedding(max_len, d_model)\n",
    "        self.text_pos_embed = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        self.vision_norm = nn.LayerNorm(d_model)\n",
    "        self.text_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.d_model = d_model  # D = 1024\n",
    "        # self.text_proj = nn.Linear(1024, self.d_model)  # ❌ Loại bỏ vì không cần nữa\n",
    "\n",
    "        self.vision_language_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.language_vision_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.vision_self_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.language_self_attn_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, vision_feats, vision_mask, text_feats, text_mask):\n",
    "        batch_size, v_len, _ = vision_feats.size()\n",
    "        _, t_len, _ = text_feats.size()\n",
    "\n",
    "        v_pos_ids = (\n",
    "            torch.arange(v_len, device=vision_feats.device)\n",
    "            .unsqueeze(0)\n",
    "            .repeat(batch_size, 1)\n",
    "        )\n",
    "        t_pos_ids = (\n",
    "            torch.arange(t_len, device=text_feats.device)\n",
    "            .unsqueeze(0)\n",
    "            .repeat(batch_size, 1)\n",
    "        )\n",
    "\n",
    "        vision_feats = self.vision_norm(vision_feats + self.vision_pos_embed(v_pos_ids))\n",
    "        text_feats = self.text_norm(text_feats + self.text_pos_embed(t_pos_ids))\n",
    "\n",
    "        for vl_attn, lv_attn, v_self, l_self in zip(\n",
    "            self.vision_language_attn_layers,\n",
    "            self.language_vision_attn_layers,\n",
    "            self.vision_self_attn_layers,\n",
    "            self.language_self_attn_layers,\n",
    "        ):\n",
    "            vision_feats = vl_attn(vision_feats, text_feats, text_feats, text_mask)\n",
    "            text_feats = lv_attn(text_feats, vision_feats, vision_feats, vision_mask)\n",
    "\n",
    "            vision_feats = v_self(vision_feats, vision_feats, vision_feats, vision_mask)\n",
    "            text_feats = l_self(text_feats, text_feats, text_feats, text_mask)\n",
    "\n",
    "        fused_feats = torch.cat([vision_feats, text_feats], dim=1)  # shape: (B, V+T, D)\n",
    "\n",
    "        return fused_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb6e91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.591859Z",
     "iopub.status.busy": "2025-05-13T04:18:59.591605Z",
     "iopub.status.idle": "2025-05-13T04:18:59.618916Z",
     "shell.execute_reply": "2025-05-13T04:18:59.618119Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.591838Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NonTextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NonTextModel, self).__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.local_visual_extractor = EfficientNetFeatureExtractor(\n",
    "            model_name=\"efficientnet_b7\"\n",
    "        ).to(self.device)\n",
    "        self.global_visual_extractor = Blip2ViTExtractor().to(self.device)\n",
    "\n",
    "        self.bart_pho = BARTPho(device=self.device, use_lora=True)  # truyền rõ device\n",
    "        self.encoder = BiDirectionalCrossAttention(max_len=1028).to(self.device)\n",
    "\n",
    "    def forward(self, images, questions, answers):\n",
    "        local_features = self.local_visual_extractor(images)\n",
    "        global_features = self.global_visual_extractor(images)\n",
    "        vision_feats = torch.cat([local_features, global_features], dim=1)\n",
    "        vision_feats = vision_feats.to(self.device)\n",
    "        vision_mask = torch.ones(vision_feats.size()[:-1], dtype=torch.bool).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        text_encoding = self.bart_pho.encode(questions)\n",
    "        text_feats = text_encoding[\"encoder_hidden_states\"]\n",
    "        question_attention_mask = text_encoding[\"attention_mask\"]\n",
    "\n",
    "        encoder_output = self.encoder(\n",
    "            vision_feats, vision_mask, text_feats, question_attention_mask\n",
    "        )\n",
    "        encoder_attention_mask = torch.cat(\n",
    "            [vision_mask, question_attention_mask], dim=1\n",
    "        )\n",
    "\n",
    "        answer_encoded = self.bart_pho.tokenizer(\n",
    "            answers, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).to(self.device)\n",
    "        answer_input_ids = answer_encoded[\"input_ids\"]\n",
    "        answer_attention_mask = answer_encoded[\"attention_mask\"]\n",
    "\n",
    "        logits = self.bart_pho.decode(\n",
    "            answer_input_ids=answer_input_ids,\n",
    "            answer_attention_mask=answer_attention_mask,\n",
    "            encoder_hidden_states=encoder_output,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "        )\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(self, images, questions):\n",
    "        # Step 1: Extract visual features\n",
    "        with torch.no_grad():\n",
    "            local_features = self.local_visual_extractor(images)  # (B, N, D)\n",
    "            global_features = self.global_visual_extractor(images)  # (B, 1, D)\n",
    "            vision_feats = torch.cat([local_features, global_features], dim=1)\n",
    "            vision_mask = torch.ones(vision_feats.size()[:-1], dtype=torch.bool).to(\n",
    "                self.device\n",
    "            )\n",
    "\n",
    "            text_encoding = self.bart_pho.encode(questions)\n",
    "            text_feats = text_encoding[\"encoder_hidden_states\"]\n",
    "            question_attention_mask = text_encoding[\"attention_mask\"]\n",
    "\n",
    "            # Step 3: Co-Attention Fusion\n",
    "            encoder_output = self.encoder(\n",
    "                vision_feats, vision_mask, text_feats, question_attention_mask\n",
    "            ).to(self.device)\n",
    "            encoder_attention_mask = torch.cat(\n",
    "                [vision_mask, question_attention_mask], dim=1\n",
    "            ).to(self.device)\n",
    "\n",
    "            return self.bart_pho.generate(encoder_output, encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714b6de-8e1c-4e85-918d-d4f82cb829f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.620007Z",
     "iopub.status.busy": "2025-05-13T04:18:59.619776Z",
     "iopub.status.idle": "2025-05-13T04:18:59.637128Z",
     "shell.execute_reply": "2025-05-13T04:18:59.636339Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.619988Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextModel, self).__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.bart_pho = BARTPho(device=self.device, use_lora=True)  # truyền rõ device\n",
    "        self.bart_pho.freeze_encoder()\n",
    "        self.encoder = BiDirectionalCrossAttention().to(self.device)\n",
    "\n",
    "    def forward(self, ocr_infos, questions, answers):\n",
    "        ocr_encoding = self.bart_pho.encode(ocr_infos)\n",
    "        ocr_feats = ocr_encoding[\"encoder_hidden_states\"]\n",
    "        ocr_attention_mask = ocr_encoding[\"attention_mask\"]\n",
    "\n",
    "        question_encoding = self.bart_pho.encode(questions)\n",
    "        question_feats = question_encoding[\"encoder_hidden_states\"]\n",
    "        question_attention_mask = question_encoding[\"attention_mask\"]\n",
    "\n",
    "        encoder_output = self.encoder(\n",
    "            ocr_feats, ocr_attention_mask, question_feats, question_attention_mask\n",
    "        )\n",
    "        encoder_attention_mask = torch.cat(\n",
    "            [ocr_attention_mask, question_attention_mask], dim=1\n",
    "        )\n",
    "\n",
    "        answer_encoded = self.bart_pho.tokenizer(\n",
    "            answers, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).to(self.device)\n",
    "        answer_input_ids = answer_encoded[\"input_ids\"]\n",
    "        answer_attention_mask = answer_encoded[\"attention_mask\"]\n",
    "\n",
    "        logits = self.bart_pho.decode(\n",
    "            answer_input_ids=answer_input_ids,\n",
    "            answer_attention_mask=answer_attention_mask,\n",
    "            encoder_hidden_states=encoder_output,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "        )\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(self, ocr_infos, questions):\n",
    "        # Step 1: Extract visual features\n",
    "        with torch.no_grad():\n",
    "            ocr_encoding = self.bart_pho.encode(ocr_infos)\n",
    "            ocr_feats = ocr_encoding[\"encoder_hidden_states\"]\n",
    "            ocr_attention_mask = ocr_encoding[\"attention_mask\"]\n",
    "\n",
    "            question_encoding = self.bart_pho.encode(questions)\n",
    "            question_feats = question_encoding[\"encoder_hidden_states\"]\n",
    "            question_attention_mask = question_encoding[\"attention_mask\"]\n",
    "\n",
    "            encoder_output = self.encoder(\n",
    "                ocr_feats, ocr_attention_mask, question_feats, question_attention_mask\n",
    "            )\n",
    "            encoder_attention_mask = torch.cat(\n",
    "                [ocr_attention_mask, question_attention_mask], dim=1\n",
    "            )\n",
    "\n",
    "            return self.bart_pho.generate(encoder_output, encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181904e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.638127Z",
     "iopub.status.busy": "2025-05-13T04:18:59.637902Z",
     "iopub.status.idle": "2025-05-13T04:18:59.658753Z",
     "shell.execute_reply": "2025-05-13T04:18:59.657998Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.638104Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_type_prompt(question: str):\n",
    "    instructions = \"\"\"\n",
    "    Bạn sẽ nhận được một câu hỏi. Hãy phân loại xem câu hỏi đó là \"text\" hoặc là \"non-text\".\n",
    "    \n",
    "    **Định nghĩa**:\n",
    "    - \"text\": bao gồm các câu hỏi dựa trên các yếu tố trích xuất từ văn bản, liên quan đến các số liệu, từ ngữ, hoặc bất kỳ hình thức văn bản nào.\n",
    "    - \"non-text\": bao gồm các câu hỏi không dựa trên các yếu tố văn bản, chẳng hạn như các câu hỏi dựa trên hình ảnh, vật thể, màu sắc, hình dạng, v.v.\n",
    "    \n",
    "    Trả lời đầu ra là một từ duy nhất: \"text\" hoặc \"non-text\".\n",
    "    \n",
    "    Dưới đây là câu hỏi cần phân loại:\n",
    "    \"\"\".strip()\n",
    "\n",
    "    ending = '\\n\\nHãy trả về 1 từ duy nhất: \"text\" hoặc \"non-text\".'\n",
    "\n",
    "    full_prompt = f\"{instructions}\\n{question}{ending}\"\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ff3af1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.660016Z",
     "iopub.status.busy": "2025-05-13T04:18:59.659506Z",
     "iopub.status.idle": "2025-05-13T04:18:59.684303Z",
     "shell.execute_reply": "2025-05-13T04:18:59.683532Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.660000Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_ocr_prompt(ocr_chunks: list[str]) -> str:\n",
    "    prompt = f\"\"\"Bạn nhận được một danh sách văn bản ngắn, trích xuất từ ảnh infographic (OCR).\n",
    "            \n",
    "    Yêu cầu:\n",
    "    - Gom nhóm các dòng có liên quan theo ngữ cảnh.\n",
    "    - Viết lại thành các câu hoàn chỉnh, ngắn gọn, rõ nghĩa dùng để thực hiện text embedding.\n",
    "    - Trả về **duy nhất một Python list hợp lệ** chứa các câu, ví dụ:\n",
    "    ```python\n",
    "    [\n",
    "        \"Câu hoàn chỉnh 1.\",\n",
    "        \"Câu hoàn chỉnh 2.\",\n",
    "        ...\n",
    "    ]\n",
    "    \n",
    "    Dữ liệu:\n",
    "    {ocr_chunks}\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30743312",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.685155Z",
     "iopub.status.busy": "2025-05-13T04:18:59.684971Z",
     "iopub.status.idle": "2025-05-13T04:18:59.700498Z",
     "shell.execute_reply": "2025-05-13T04:18:59.699622Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.685141Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fix_list_string(raw_str: str) -> str:\n",
    "    # Bước 1: Dọn sạch chuỗi đầu vào\n",
    "    raw_str = raw_str.strip()\n",
    "\n",
    "    # Bước 2: Chèn dấu phẩy giữa các dấu \" liền nhau nếu có\n",
    "    raw_str = re.sub(r'\"\\s*\"', '\", \"', raw_str)\n",
    "\n",
    "    # Bước 3: Parse từng dòng\n",
    "    lines = raw_str.strip(\"[]\").split(\"\\n\")\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip().rstrip(\",\")\n",
    "\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Loại bỏ dấu \" bao ngoài nếu có\n",
    "        if line.startswith('\"') and line.endswith('\"'):\n",
    "            line = line[1:-1]\n",
    "\n",
    "        # Escape các dấu \" bên trong nội dung\n",
    "        line = line.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n",
    "\n",
    "        # Thêm lại dấu \" bao ngoài\n",
    "        line = f'\"{line}\"'\n",
    "\n",
    "        # Thêm dấu phẩy nếu chưa phải dòng cuối\n",
    "        if i < len(lines) - 1:\n",
    "            line += \",\"\n",
    "\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    fixed_str = \"[\\n\" + \"\\n\".join(cleaned_lines) + \"\\n]\"\n",
    "    return fixed_str\n",
    "\n",
    "\n",
    "def clean_ocr_contents(response: str) -> str:\n",
    "    response = re.sub(r\"^```python\\s*\", \"\", response.strip(), flags=re.IGNORECASE)\n",
    "    response = re.sub(r\"\\s*```$\", \"\", response.strip())\n",
    "    # Thay dấu ngoặc kép cong thành thẳng\n",
    "    response = response.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    # Thực thi parsing\n",
    "    response = fix_list_string(response)\n",
    "    meaning_sentences = ast.literal_eval(response)\n",
    "    joined_text = \" \".join(meaning_sentences)\n",
    "\n",
    "    return joined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed292c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:18:59.701684Z",
     "iopub.status.busy": "2025-05-13T04:18:59.701336Z",
     "iopub.status.idle": "2025-05-13T04:20:55.224834Z",
     "shell.execute_reply": "2025-05-13T04:20:55.224053Z",
     "shell.execute_reply.started": "2025-05-13T04:18:59.701659Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b7_lukemelas-c5b4e57e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b7_lukemelas-c5b4e57e.pth\n",
      "100%|██████████| 255M/255M [00:02<00:00, 93.6MB/s] \n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04426ac70be642acb0718735743346f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e174aeabc294ced9dc70283c439b092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a5f6ca82a74820a1ba5a005010e6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.89G/1.89G [00:47<00:00, 42.7MB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/lavis/models/eva_vit.py:433: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cached_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa5b30137c845cab52e3ee2c09f623d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 712M/712M [00:17<00:00, 43.2MB/s] \n",
      "/usr/local/lib/python3.11/dist-packages/lavis/models/blip2_models/blip2.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(cached_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce371cc8acaa483a97f0cb82422ac245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/897 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17eb00fef21b4db3b39dae579f9345c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb114946e71495d90fe5beaca3cf228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dict.txt:   0%|          | 0.00/360k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a276011f40b403ab69b949c7045204e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.58G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "/tmp/ipykernel_31/4157559533.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath, map_location=DEVICE)\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, filepath):\n",
    "    #\n",
    "    if os.path.isfile(filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=DEVICE)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "\n",
    "#\n",
    "TEXT_MODEL = TextModel().to(DEVICE)\n",
    "load_checkpoint(TEXT_MODEL, TEXT_CHECKPOINT_PATH)\n",
    "TEXT_MODEL.eval()\n",
    "\n",
    "#\n",
    "NONTEXT_MODEL = NonTextModel().to(DEVICE)\n",
    "load_checkpoint(NONTEXT_MODEL, NONTEXT_CHECKPOINT_PATH)\n",
    "NONTEXT_MODEL.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af900c19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:20:55.230435Z",
     "iopub.status.busy": "2025-05-13T04:20:55.230192Z",
     "iopub.status.idle": "2025-05-13T04:20:55.250158Z",
     "shell.execute_reply": "2025-05-13T04:20:55.249529Z",
     "shell.execute_reply.started": "2025-05-13T04:20:55.230414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_answer(image: Image, question: str) -> str:\n",
    "    #\n",
    "    prompt = build_type_prompt(question)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    question_type = CLIENT.models.generate_content(\n",
    "        model=GEMINI_MODEL, contents=prompt, config=GENERATION_CONFIG\n",
    "    ).text.strip()\n",
    "    print(f\"Question types: {question_type}\")\n",
    "\n",
    "    #\n",
    "    if question_type.lower() == \"text\":\n",
    "        #\n",
    "        image_np = np.array(image)\n",
    "        ocr_chunks = OCR_READER.readtext(image_np)\n",
    "        ocr_chunks = list(chunk[1] for chunk in ocr_chunks)\n",
    "\n",
    "        #\n",
    "        ocr_prompt = build_ocr_prompt(ocr_chunks)\n",
    "        contents = [ocr_prompt, image]\n",
    "\n",
    "        try:\n",
    "            #\n",
    "            response = CLIENT.models.generate_content(\n",
    "                model=GEMINI_MODEL, contents=contents, config=GENERATION_CONFIG\n",
    "            )\n",
    "            response = response.text\n",
    "            ocr_info = clean_ocr_contents(response)\n",
    "            print(f\"OCR \")\n",
    "        except:\n",
    "            return (\n",
    "                \"Your image may contain sensitive or violent content, please change it!\"\n",
    "            )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            answer = TEXT_MODEL.generate([ocr_info], [question])\n",
    "            print(f\"Text model responded\")\n",
    "        return answer\n",
    "    elif question_type.lower() == \"non-text\":\n",
    "        with torch.no_grad():\n",
    "            answer = NONTEXT_MODEL.generate([image], [question])\n",
    "            print(f\"Non-text model responded\")\n",
    "        return answer\n",
    "    else:\n",
    "        return \"Please try again!\"\n",
    "\n",
    "\n",
    "# def generate_answer(image: Image, question: str) -> str:\n",
    "#     with torch.no_grad():\n",
    "#         answer = NONTEXT_MODEL.generate([image], [question])\n",
    "#         print(f\"Non-text model responded\")\n",
    "#     return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d375a-c9e0-4c2a-b37b-b43f60d78ec1",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf39b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q fastapi uvicorn nest-asyncio pyngrok pillow\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d3fe6-7da2-4b15-a041-30b5fd10d2d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:20:59.226090Z",
     "iopub.status.busy": "2025-05-13T04:20:59.225883Z",
     "iopub.status.idle": "2025-05-13T04:21:00.918860Z",
     "shell.execute_reply": "2025-05-13T04:21:00.917909Z",
     "shell.execute_reply.started": "2025-05-13T04:20:59.226069Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n"
     ]
    }
   ],
   "source": [
    "!ngrok config add-authtoken <YOUR_NGROK_TOKEN_KEY_HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16747461-2fec-4baa-80a3-4692f2337c09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:21:00.920832Z",
     "iopub.status.busy": "2025-05-13T04:21:00.920075Z",
     "iopub.status.idle": "2025-05-13T04:21:01.169157Z",
     "shell.execute_reply": "2025-05-13T04:21:01.168390Z",
     "shell.execute_reply.started": "2025-05-13T04:21:00.920804Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import gc\n",
    "import base64\n",
    "from PIL import Image\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "# Allow nested async event loop (required for Jupyter/Kaggle)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "class InferenceRequest(BaseModel):\n",
    "    questions: list[str]\n",
    "    image_base64: str\n",
    "\n",
    "\n",
    "class InferenceResponse(BaseModel):\n",
    "    answers: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd843e1-61be-46f2-a15d-4a28cf1c5781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T04:21:01.172367Z",
     "iopub.status.busy": "2025-05-13T04:21:01.172146Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [31]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NgrokTunnel: \"https://204d-34-141-197-202.ngrok-free.app\" -> \"http://localhost:8000\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n"
     ]
    }
   ],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "# Enable CORS for development (allow all)\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "def decode_image(base64_string: str) -> Image:\n",
    "    try:\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        image = Image.open(io.BytesIO(image_data)).convert(\"RGB\")\n",
    "        print(f\"Image decoded\")\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"Invalid base64 image input\")\n",
    "\n",
    "\n",
    "@app.post(\"/infer\", response_model=InferenceResponse)\n",
    "def infer(request: InferenceRequest):\n",
    "    try:\n",
    "        # print(request.question)\n",
    "        image = decode_image(request.image_base64)\n",
    "\n",
    "        result = generate_answer(image, request.questions[0])\n",
    "        print(result)\n",
    "        del image\n",
    "        return InferenceResponse(answers=result)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "    finally:\n",
    "        # Explicit memory cleanup\n",
    "        gc.collect()  # Run Python garbage collection\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()  # Free up CUDA memory\n",
    "\n",
    "\n",
    "try:\n",
    "    ngrok.kill()\n",
    "    public_url = ngrok.connect(8000)\n",
    "    print(f\"{public_url}\")\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "except (KeyboardInterrupt, Exception):\n",
    "    ..."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7383612,
     "sourceId": 11761392,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
